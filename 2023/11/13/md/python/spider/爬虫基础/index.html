<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>吕小布の博客 | 吕小布の博客</title><meta name="keywords" content="吕小布;博客;生活记录"><meta name="author" content="Sai"><meta name="copyright" content="Sai"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="[TOC] 教程来源 1.爬虫简介抓取互联网上的数据，为我所用。 有了大量的数据，就如同有了一个数据银行一样。 下一步做的就是如何将这些爬取的数据，产品化、商业化。 1.1.爬虫合法性探究1.1.1.爬虫究竟是违法还是合法的？ 在法律中不被禁止 具有违法风险 区分为善意爬虫和恶意爬虫  1.1.2.爬虫带来的风险 爬虫干扰了被访问网站的正常运营 爬虫抓取了受到法律保护的特定类型的数据或信息  1.">
<meta property="og:type" content="article">
<meta property="og:title" content="吕小布の博客">
<meta property="og:url" content="https://mindcons.cn/2023/11/13/md/python/spider/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="吕小布の博客">
<meta property="og:description" content="[TOC] 教程来源 1.爬虫简介抓取互联网上的数据，为我所用。 有了大量的数据，就如同有了一个数据银行一样。 下一步做的就是如何将这些爬取的数据，产品化、商业化。 1.1.爬虫合法性探究1.1.1.爬虫究竟是违法还是合法的？ 在法律中不被禁止 具有违法风险 区分为善意爬虫和恶意爬虫  1.1.2.爬虫带来的风险 爬虫干扰了被访问网站的正常运营 爬虫抓取了受到法律保护的特定类型的数据或信息  1.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-11-12T23:54:27.798Z">
<meta property="article:modified_time" content="2023-12-05T23:38:18.394Z">
<meta property="article:author" content="Sai">
<meta property="article:tag" content="吕小布;博客;生活记录">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://mindcons.cn/2023/11/13/md/python/spider/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"61301LMFP2","apiKey":"a7627d15f4af23df7fbc3e2922471858","indexName":"mindcons","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":350},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Sai","link":"链接: ","source":"来源: 吕小布の博客","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '吕小布の博客',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-06 07:38:18'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/custom/css/custom.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://bu.dusays.com/2023/02/10/63e583843360e.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">178</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">52</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fa-sharp fa-solid fa-photo-film"></i><span> 相册</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-video"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/booklist/"><i class="fa-fw fa-solid fa-book"></i><span> 书单</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/sponsorWall/"><i class="fa-fw fas fa-money-check-alt"></i><span> 赞助墙</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">吕小布の博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-book"></i><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group hide" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fa-sharp fa-solid fa-photo-film"></i><span> 相册</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fas fa-video"></i><span> 番剧</span></a></li><li><a class="site-page child" href="/booklist/"><i class="fa-fw fa-solid fa-book"></i><span> 书单</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/sponsorWall/"><i class="fa-fw fas fa-money-check-alt"></i><span> 赞助墙</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fas fa-comments"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">无题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-12T23:54:27.798Z" title="发表于 2023-11-13 07:54:27">2023-11-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-05T23:38:18.394Z" title="更新于 2023-12-06 07:38:18">2023-12-06</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>[TOC]</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Yh411o7Sz?p=14">教程来源</a></p>
<h2 id="1-爬虫简介"><a href="#1-爬虫简介" class="headerlink" title="1.爬虫简介"></a>1.爬虫简介</h2><p>抓取互联网上的数据，为我所用。</p>
<p>有了大量的数据，就如同有了一个数据银行一样。</p>
<p>下一步做的就是如何将这些爬取的数据，产品化、商业化。</p>
<h3 id="1-1-爬虫合法性探究"><a href="#1-1-爬虫合法性探究" class="headerlink" title="1.1.爬虫合法性探究"></a>1.1.爬虫合法性探究</h3><h4 id="1-1-1-爬虫究竟是违法还是合法的？"><a href="#1-1-1-爬虫究竟是违法还是合法的？" class="headerlink" title="1.1.1.爬虫究竟是违法还是合法的？"></a>1.1.1.爬虫究竟是违法还是合法的？</h4><ul>
<li>在法律中不被禁止</li>
<li>具有违法风险</li>
<li>区分为善意爬虫和恶意爬虫</li>
</ul>
<h4 id="1-1-2-爬虫带来的风险"><a href="#1-1-2-爬虫带来的风险" class="headerlink" title="1.1.2.爬虫带来的风险"></a>1.1.2.爬虫带来的风险</h4><ul>
<li>爬虫干扰了被访问网站的正常运营</li>
<li>爬虫抓取了受到法律保护的特定类型的数据或信息</li>
</ul>
<h4 id="1-1-3-如何避免进局子喝茶"><a href="#1-1-3-如何避免进局子喝茶" class="headerlink" title="1.1.3.如何避免进局子喝茶"></a>1.1.3.如何避免进局子喝茶</h4><ul>
<li>时常优化自己的程序，避免干扰被访问网站的正常运行</li>
<li>在使用、传播爬取到的数据时，审查抓取的内容，如果发现了涉及到用户以及商业机密等敏感内容时，需要及时停止爬取或传播。</li>
</ul>
<h3 id="1-2-爬虫初始深入"><a href="#1-2-爬虫初始深入" class="headerlink" title="1.2.爬虫初始深入"></a>1.2.爬虫初始深入</h3><h4 id="1-2-1-爬虫在使用场景中的分类"><a href="#1-2-1-爬虫在使用场景中的分类" class="headerlink" title="1.2.1.爬虫在使用场景中的分类"></a>1.2.1.爬虫在使用场景中的分类</h4><ul>
<li>通用爬虫<ul>
<li>抓取系统的重要组成部分。</li>
<li>抓取的是一整张页面数据。</li>
</ul>
</li>
<li>聚焦爬虫<ul>
<li>是建立在通用爬虫的基础之上。</li>
<li>抓取的是页面中特定的局部内容。</li>
</ul>
</li>
<li>增量式爬虫<ul>
<li>检测网站中数据更新的情况。</li>
<li>只会抓取网站中最新更新出来的数据。</li>
</ul>
</li>
</ul>
<h4 id="1-2-2-爬虫的矛与盾"><a href="#1-2-2-爬虫的矛与盾" class="headerlink" title="1.2.2.爬虫的矛与盾"></a>1.2.2.爬虫的矛与盾</h4><ul>
<li>反爬机制<ul>
<li>门户网站，可以通过指定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。</li>
</ul>
</li>
<li>反反爬策略<ul>
<li>爬虫程序，可以通过指定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站的和数据。</li>
</ul>
</li>
</ul>
<h4 id="1-2-3-robots-txt协议"><a href="#1-2-3-robots-txt协议" class="headerlink" title="1.2.3.robots.txt协议"></a>1.2.3.robots.txt协议</h4><p>君子协议，规定了网站中哪些数据可以被爬虫爬取，哪些数据不可以被爬取。</p>
<p><a target="_blank" rel="noopener" href="http://www.baidu.com/robots.txt">http://www.baidu.com/robots.txt</a></p>
<h2 id="2-http-amp-https协议"><a href="#2-http-amp-https协议" class="headerlink" title="2.http&amp;https协议"></a>2.http&amp;https协议</h2><p>详细请点击<a href="">http&amp;https协议</a></p>
<h3 id="2-1-http协议"><a href="#2-1-http协议" class="headerlink" title="2.1.http协议"></a>2.1.http协议</h3><ul>
<li>概念：就是服务器和客户端，进行数据交互的一种形式。</li>
<li>常用请求头信息：<ul>
<li>User-Agent：请求头的身份标识。</li>
<li>Connection：请求完毕后，是断开连接还是保持连接。</li>
</ul>
</li>
<li>常用响应头信息：<ul>
<li>Content-Type：服务器响应回客户端的数据类型。</li>
</ul>
</li>
</ul>
<h3 id="2-2-https协议"><a href="#2-2-https协议" class="headerlink" title="2.2.https协议"></a>2.2.https协议</h3><ul>
<li>概念：安全的超文本传输协议。</li>
<li>加密方式：<ul>
<li>对称密钥加密</li>
<li>非对称密钥加密</li>
<li>证书密钥加密</li>
</ul>
</li>
</ul>
<h2 id="3-Requests模块"><a href="#3-Requests模块" class="headerlink" title="3.Requests模块"></a>3.Requests模块</h2><p>requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。</p>
<p>作用：模拟浏览器发送请求。</p>
<p>如何使用：(requests模块的编码流程)：</p>
<ul>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>持久化存储</li>
</ul>
<p>环境安装：<code>pip install requests</code></p>
<p>实战：获取sogou首页的数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requestsif __name__ == <span class="string">&#x27;__main__&#x27;</span>:    <span class="comment"># 指定url    url = &#x27;https://wwww.sogou.com/&#x27;    # 发送get请求，获取响应数据    response = requests.get(url = url)    # 将响应数据转化为字符串    response_text = response.text    # 本地持久化存储    with open(&#x27;sogou.html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fw:        fw.write(response_text)</span></span><br></pre></td></tr></table></figure>

<h3 id="3-1-Requests巩固"><a href="#3-1-Requests巩固" class="headerlink" title="3.1.Requests巩固"></a>3.1.Requests巩固</h3><h4 id="3-1-1-深入案例介绍"><a href="#3-1-1-深入案例介绍" class="headerlink" title="3.1.1.深入案例介绍"></a>3.1.1.深入案例介绍</h4><ul>
<li>爬取搜狗指定词条，对应的搜索结果页面（简易网页采集器）。</li>
<li>破解百度翻译。</li>
<li>爬取豆瓣电影分类排行榜中的电影详情数据。</li>
<li>爬取肯德基餐厅查询中，指定地点的餐厅数。</li>
<li>爬取国家药品监督管理总局基于中华人民共和国化妆品生产许可证相关数据。</li>
</ul>
<h4 id="3-1-2-简易网页采集器"><a href="#3-1-2-简易网页采集器" class="headerlink" title="3.1.2.简易网页采集器"></a>3.1.2.简易网页采集器</h4><ul>
<li>UA检测</li>
<li>UA伪装</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requestsif __name__ == <span class="string">&quot;__main__&quot;</span>:    url = <span class="string">&#x27;https://www.sogou.com/web&#x27;</span>    headers = &#123;        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>    &#125;    <span class="comment"># 处理url携带的参数：封装到字典中    kw = input(&#x27;enter a keyword:&#x27;)    param = &#123;        &#x27;query&#x27;: kw    &#125;    response = requests.get(url = url,params = param,headers = headers)    page_text = response.text    with open(kw+&#x27;html&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;)as fw:        fw.write(page_text)    # print(page_text)    </span></span><br></pre></td></tr></table></figure>

<h4 id="3-1-3-百度翻译"><a href="#3-1-3-百度翻译" class="headerlink" title="3.1.3.百度翻译"></a>3.1.3.百度翻译</h4><ul>
<li>post请求（携带了参数）</li>
<li>响应是一组Json数据<ul>
<li><code>resposne.json()</code>直接返回的是obj</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:    post_url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span>    headers =&#123;        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>    &#125;    <span class="comment"># post请求参数处理（同get请求一致）    input_string = input(&#x27;enter a key:&#x27;)    data = &#123;        &#x27;kw&#x27;: input_string    &#125;    response = requests.post(url = post_url,data = data,headers=headers)    dict_obj = response.json()    # 持久化存储    with open(input_string +&#x27;.json&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fw:        json.dump(dict_obj,fp=fw,ensure_ascii=False)    print(dict_obj)</span></span><br></pre></td></tr></table></figure>



<h4 id="3-1-4-豆瓣电影"><a href="#3-1-4-豆瓣电影" class="headerlink" title="3.1.4.豆瓣电影"></a>3.1.4.豆瓣电影</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport json <span class="keyword">if</span> __name__ ==<span class="string">&#x27;__main__&#x27;</span>:    url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list&#x27;</span>    headers =&#123;        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>    &#125;    param = &#123;        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;24&#x27;</span>,        <span class="string">&#x27;interval_id&#x27;</span>: <span class="string">&#x27;100:90&#x27;</span>,        <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;&#x27;</span>,        <span class="string">&#x27;start&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,        <span class="string">&#x27;limit&#x27;</span>: <span class="string">&#x27;20&#x27;</span>,    &#125;    response = requests.get(url = url,params = param,headers=headers)    <span class="built_in">print</span>(response)    list_data = response.json()    <span class="comment"># 持久化存储    with open(&#x27;movie.json&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;) as fw:        json.dump(list_data,fp=fw,ensure_ascii=False)    print(list_data)</span></span><br></pre></td></tr></table></figure>

<p>备注：会被限制ip，response返回的是403</p>
<h4 id="3-1-5-作业"><a href="#3-1-5-作业" class="headerlink" title="3.1.5.作业"></a>3.1.5.作业</h4><p>肯德基餐厅</p>
<p>观测地址栏的url有没有发生变化，如果没有发生变化，但是数据发生了更新，表示发送了ajax请求。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:    url = <span class="string">&#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#x27;</span>    headers =&#123;        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>    &#125;    data = &#123;        <span class="string">&#x27;cname&#x27;</span>:<span class="string">&#x27;&#x27;</span>,         <span class="string">&#x27;pid&#x27;</span>:<span class="string">&#x27;&#x27;</span>,        <span class="string">&#x27;keyword&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,        <span class="string">&#x27;pageIndex&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span>,    &#125;    response = requests.post(url = url,data = data,headers=headers)    response_text = response.text    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;kfc.txt&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:        fw.write(response_text )</span><br></pre></td></tr></table></figure>

<p>备注：这里请求的url，不要去掉后面的参数</p>
<h3 id="3-2-综合练习"><a href="#3-2-综合练习" class="headerlink" title="3.2.综合练习"></a>3.2.综合练习</h3><h4 id="3-2-1-药监总局01"><a href="#3-2-1-药监总局01" class="headerlink" title="3.2.1.药监总局01"></a>3.2.1.药监总局01</h4><ul>
<li>url的域名都是一样的，只有携带的参数（id）不一样</li>
<li>id值可以从首页对应的ajax请求到的json串中获取</li>
</ul>
<p><code>settings.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">START_URL = &#123;	<span class="string">&#x27;home&#x27;</span>: <span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList&#x27;</span>,	<span class="string">&#x27;detail&#x27;</span>: <span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById&#x27;</span>,&#125;DATA = &#123;	<span class="string">&#x27;home&#x27;</span>: &#123;		<span class="string">&quot;on&quot;</span>: <span class="string">&quot;true&quot;</span>,		<span class="string">&quot;page&quot;</span>: <span class="string">&quot;1&quot;</span>,		<span class="string">&quot;pageSize&quot;</span>: <span class="string">&quot;15&quot;</span>,		<span class="string">&quot;productName&quot;</span>: <span class="string">&quot;&quot;</span>,		<span class="string">&quot;conditionType&quot;</span>: <span class="string">&quot;1&quot;</span>,		<span class="string">&quot;applyname&quot;</span>: <span class="string">&quot;&quot;</span>,		<span class="string">&quot;applysn&quot;</span>: <span class="string">&quot;&quot;</span>,	&#125;,	<span class="string">&#x27;detail&#x27;</span>: &#123;		<span class="string">&quot;id&quot;</span>: <span class="string">&quot;ef765ed313ec457dbecd434aa08b0e93&quot;</span>,	&#125;,&#125;HEADERS = &#123;	<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>

<p><code>utils.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jsondef json_local_storage(response, outfile = <span class="string">&#x27;local_storage.json&#x27;</span>):    response_json = response.json()    <span class="keyword">with</span> <span class="built_in">open</span>(outfile,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:        json.dump(response_json,fp=fw,ensure_ascii=<span class="literal">False</span>)<span class="keyword">def</span> <span class="title function_">txt_local_storage</span>(<span class="params">response, outfile = <span class="string">&#x27;txt_strage.txt&#x27;</span></span>):    response_text = response.text    <span class="keyword">with</span> <span class="built_in">open</span>(outfile, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fw:        fw.write(response_text)<span class="keyword">def</span> <span class="title function_">redis_conn</span>(<span class="params">host,port,password</span>):    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p><code>spider_task.py</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonfrom settings <span class="keyword">import</span> START_URL, HEADERS, DATAfrom utils <span class="keyword">import</span> json_local_storage,txt_local_storageif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:    response = requests.post(url = START_URL[<span class="string">&#x27;home&#x27;</span>],data = DATA[<span class="string">&#x27;home&#x27;</span>],headers=HEADERS)    <span class="comment"># json_local_storage(&#x27;home.json&#x27;)    response_json = response.json()    company_array = response_json[&#x27;list&#x27;]    for ele in company_array:        time.sleep(1)        # print(ele[&#x27;ID&#x27;])        DATA[&#x27;detail&#x27;][&#x27;id&#x27;] = ele[&#x27;ID&#x27;]        print(DATA[&#x27;detail&#x27;])        response_detail = requests.post(url = START_URL[&#x27;detail&#x27;],data = DATA[&#x27;detail&#x27;],headers=HEADERS)                json_local_storage(response_detail, ele[&#x27;EPS_NAME&#x27;] + &#x27;.json&#x27;)        # print(response_detail)        time.sleep(1)</span></span><br></pre></td></tr></table></figure>

<h2 id="4-数据解析概述"><a href="#4-数据解析概述" class="headerlink" title="4.数据解析概述"></a>4.数据解析概述</h2><p>聚焦爬虫：爬取页面中指定的页面内容</p>
<p>编码流程：</p>
<ul>
<li>指定url</li>
<li>发起请求</li>
<li>获取响应数据</li>
<li>数据解析</li>
<li>持久化存储</li>
</ul>
<p>数据解析分类：</p>
<ul>
<li>正则</li>
<li>bs4</li>
<li>xpath</li>
</ul>
<p>数据解析原理概述：</p>
<ul>
<li>解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储</li>
<li>进行指定标签的定位</li>
<li>标签或者标签对应的属性中存储的数据值进行提取（解析）</li>
</ul>
<p>响应数据的类型：</p>
<ul>
<li><code>response.text</code>，处理字符串类型的响应数据</li>
<li><code>response.content</code>，处理二进制类型的响应数据</li>
<li><code>response.json()</code>，处理Json类型（对象类型）的响应数据</li>
</ul>
<h3 id="4-1-图片数据爬取"><a href="#4-1-图片数据爬取" class="headerlink" title="4.1.图片数据爬取"></a>4.1.图片数据爬取</h3><p>单张图片数据爬取</p>
<p>spider_task.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonfrom settings <span class="keyword">import</span> START_URL, HEADERS, DATAfrom utils.utils_strorage <span class="keyword">import</span> content_local_storageif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:        response = requests.get(url=START_URL[<span class="string">&#x27;pic_demo&#x27;</span>],headers=HEADERS)    content_local_storage(response,<span class="string">&#x27;qiutu.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>utils_storage.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 二进制类型响应对象的本地存储def content_local_storage(response, outfile = &#x27;content_strage.jpg&#x27;):    response_content = response.content	    # 二进制存储不需要指定编码格式    with open(outfile, &#x27;wb&#x27;) as fw:        fw.write(response_content)</span></span><br></pre></td></tr></table></figure>

<h3 id="4-2-正则解析"><a href="#4-2-正则解析" class="headerlink" title="4.2.正则解析"></a>4.2.正则解析</h3><p>单页面的多张图片爬取</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonfrom settings <span class="keyword">import</span> START_URL, HEADERS, DATAfrom utils.utils_strorage <span class="keyword">import</span> content_local_storage,txt_local_storagefrom utils.utils_os <span class="keyword">import</span> my_mkdirimport reif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:    my_mkdir(<span class="string">&#x27;./qiutu&#x27;</span>)        response = requests.get(url=START_URL[<span class="string">&#x27;pic_demo&#x27;</span>],headers=HEADERS)    response_text = response.text    ex = <span class="string">&#x27;&lt;div class=&quot;thumb&quot;&gt;.*?&lt;img src=&quot;(.*?)&quot; alt.*?&lt;/div&gt;&#x27;</span>    src_list = re.findall(ex,response_text,re.S)    <span class="keyword">for</span> src <span class="keyword">in</span> src_list:        src = <span class="string">&#x27;https:&#x27;</span> + src        time.sleep(<span class="number">1</span>)        response = requests.get(url=src,headers=HEADERS)        img_name = src.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]        img_path = <span class="string">&#x27;./qiutu/&#x27;</span>+ img_name                content_local_storage(response,img_path)        time.sleep(<span class="number">1</span>)    </span><br></pre></td></tr></table></figure>

<p>多页面，多图片爬取</p>
<ul>
<li>设置url请求的通用模板</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests <span class="keyword">import</span> timeimport jsonfrom settings <span class="keyword">import</span> START_URL, HEADERS, DATAfrom utils.utils_strorage <span class="keyword">import</span> content_local_storage,txt_local_storagefrom utils.utils_os <span class="keyword">import</span> my_mkdirimport refrom utils.utils_parse <span class="keyword">import</span> regex_parseif __name__ ==<span class="string">&#x27;__main__&#x27;</span>:        <span class="keyword">def</span> <span class="title function_">single_parse</span>():        my_mkdir(<span class="string">&#x27;./qiutu&#x27;</span>)        response = requests.get(url=START_URL[<span class="string">&#x27;pic_demo&#x27;</span>][<span class="string">&#x27;index_url&#x27;</span>],headers=HEADERS)        ex = <span class="string">&#x27;&lt;div class=&quot;thumb&quot;&gt;.*?&lt;img src=&quot;(.*?)&quot; alt.*?&lt;/div&gt;&#x27;</span>        src_list = regex_parse(ex,response,re.S)        <span class="keyword">for</span> src <span class="keyword">in</span> src_list:            src = <span class="string">&#x27;https:&#x27;</span> + src            time.sleep(<span class="number">1</span>)            response = requests.get(url=src,headers=HEADERS)            img_name = src.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]            img_path = <span class="string">&#x27;./qiutu/&#x27;</span>+ img_name                        content_local_storage(response,img_path)            time.sleep(<span class="number">1</span>)        <span class="keyword">def</span> <span class="title function_">multi_parse</span>():        template_url = START_URL[<span class="string">&#x27;pic_demo&#x27;</span>][<span class="string">&#x27;template_url&#x27;</span>]        my_mkdir(<span class="string">&#x27;./qiutu&#x27;</span>)        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">3</span>):            next_url = template_url % i            response = requests.get(url=next_url,headers=HEADERS)            ex = <span class="string">&#x27;&lt;div class=&quot;thumb&quot;&gt;.*?&lt;img src=&quot;(.*?)&quot; alt.*?&lt;/div&gt;&#x27;</span>            src_list = regex_parse(ex,response,re.S)            <span class="keyword">for</span> src <span class="keyword">in</span> src_list:                src = <span class="string">&#x27;https:&#x27;</span> + src                time.sleep(<span class="number">1</span>)                response = requests.get(url=src,headers=HEADERS)                img_name = src.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]                img_path = <span class="string">&#x27;./qiutu/&#x27;</span>+ img_name                                content_local_storage(response,img_path)                time.sleep(<span class="number">1</span>)            <span class="built_in">print</span>(next_url+<span class="string">&#x27;start&#x27;</span>)    multi_parse()    </span><br></pre></td></tr></table></figure>

<h3 id="4-3-bs4解析"><a href="#4-3-bs4解析" class="headerlink" title="4.3.bs4解析"></a>4.3.bs4解析</h3><h4 id="4-3-1-bs4解析概述"><a href="#4-3-1-bs4解析概述" class="headerlink" title="4.3.1.bs4解析概述"></a>4.3.1.bs4解析概述</h4><p>bs4只可以用在python中</p>
<ul>
<li><p>数据解析的原理</p>
<ul>
<li>1.标签定位</li>
<li>2.提取标签、标签属性中存储数据</li>
</ul>
</li>
<li><p>bs4数据解析的原理：</p>
<ul>
<li>1.实例化一个BeautifulSoup对象，并且将页面元数据加载到该对象中</li>
<li>2.通过调用BeautifulSoup对象中的相关的属性或者方法进行标签定位和数据提取</li>
</ul>
</li>
<li><p>环境安装</p>
<ul>
<li><code>pip install bs4</code></li>
<li><code>pip install lxml</code></li>
</ul>
</li>
<li><p>如何实例化BeautifulSoup</p>
<ul>
<li><p>导包：<code>from bs4 import BeautifulSoup</code></p>
</li>
<li><p>对象的实例化</p>
<ul>
<li><p>1.将本地的html加载到BeautifulSoup对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将本地的html文档中的数据加载到该对象中with open(&#x27;aa.html&#x27;,&#x27;r&#x27;,encoding=&#x27;utf-8&#x27;) as fr:    soup = BeautifulSoup(fr,&#x27;lxml&#x27;)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>2.将互联网上获取的页面源码加载到该对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">page_text = response.textsoup = BeautifulSoup(page_text,<span class="string">&#x27;lxml&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-3-2-bs4解析具体使用讲解"><a href="#4-3-2-bs4解析具体使用讲解" class="headerlink" title="4.3.2.bs4解析具体使用讲解"></a>4.3.2.bs4解析具体使用讲解</h4><ul>
<li>提供的用于数据解析的方法和属性<ul>
<li><code>soup.tagName</code>，等价于<code>soup.find(&#39;tagName&#39;)</code><ul>
<li>返回的是html中第一次出现的tagName标签</li>
</ul>
</li>
<li>属性定位<ul>
<li><code>soup.find(&#39;div&#39;,class_=&#39;song&#39;)</code>，返回类名为song对应的div，记得class要加下划线</li>
<li><code>soup.findAll(&#39;tagName&#39;)</code>，返回所有的tag标签，返回值为数组</li>
</ul>
</li>
<li>标签选择器（和CSS选择器一致），返回的是列表<ul>
<li><code>select(&#39;.song&#39;)</code>，类选择器</li>
<li><code>select(&#39;#song&#39;)</code>，id选择器</li>
<li><code>select(&#39;a&#39;)</code>，标签选择器</li>
<li>层级选择器<ul>
<li><code>select(&#39;.tag &gt; ul &gt; li &gt; a&#39;)</code>，单个多层级用<code>&gt;</code>表示</li>
<li><code>select(&#39;.tang &gt; ul a)</code>，多个层级用空格表示</li>
</ul>
</li>
<li>层级选择器中，不支持索引定位</li>
</ul>
</li>
</ul>
</li>
<li>获取标签之间的文本数据<ul>
<li><code>soup.a.text/string/get_text()</code></li>
<li>区别<ul>
<li><code>text/get_text()</code>，可以获取某一个标签中的所有文本内容</li>
<li><code>string</code>，只可以获取该标签下面直系的文本内容</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-3-3-bs4解析案例实战"><a href="#4-3-3-bs4解析案例实战" class="headerlink" title="4.3.3.bs4解析案例实战"></a>4.3.3.bs4解析案例实战</h4><p>响应数据乱码解决方案</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response_text = response.text.encode(<span class="string">&#x27;ISO-8859-1&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sanguo</span>():    response = requests.get(url=START_URL[<span class="string">&#x27;sanguo&#x27;</span>],headers=HEADERS)    response_text = response.text.encode(<span class="string">&#x27;ISO-8859-1&#x27;</span>)    soup = BeautifulSoup(response_text,<span class="string">&#x27;lxml&#x27;</span>)    li_list = soup.select(<span class="string">&#x27;.book-mulu &gt; ul &gt; li&#x27;</span>)    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:        title = li.a.string        detail_url = <span class="string">&#x27;https://www.shicimingju.com&#x27;</span> + li.a[<span class="string">&#x27;href&#x27;</span>]        <span class="built_in">print</span>(detail_url)        detail_response = requests.get(url=detail_url,headers=HEADERS)        detail_response_text = detail_response.text.encode(<span class="string">&#x27;ISO-8859-1&#x27;</span>)        soup_detail = BeautifulSoup(detail_response_text,<span class="string">&#x27;lxml&#x27;</span>)        detail_tag = soup_detail.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;chapter_content&#x27;</span>)        detail_content = detail_tag.text        <span class="built_in">print</span>(detail_content)        <span class="keyword">break</span>        time.sleep(<span class="number">1</span>)sanguo()</span><br></pre></td></tr></table></figure>

<h3 id="4-4-xpath解析"><a href="#4-4-xpath解析" class="headerlink" title="4.4.xpath解析"></a>4.4.xpath解析</h3><h4 id="4-4-1-xpath解析基础"><a href="#4-4-1-xpath解析基础" class="headerlink" title="4.4.1.xpath解析基础"></a>4.4.1.xpath解析基础</h4><p>xpath解析：最常用且最便捷高效的一种解析方法，通用性。</p>
<p>xpath解析原理</p>
<ul>
<li>1.实例化一个etree对象，且需要将被解析的页面源码数据加载到该对象中</li>
<li>2.调用etree对象中的xpath方法，结合着xpath表达式是实现标签的定位和内容的捕获</li>
</ul>
<p>环境的安装</p>
<ul>
<li><code>pip install lxml</code></li>
</ul>
<p>如何实例化一个etree对象</p>
<ul>
<li><p>1.将本地的html文档中的源码数据加载到etree对象中</p>
<p><code>etree.parse(filepath)</code></p>
</li>
<li><p>2.可以将从互联网上获取到的源码数据加载到该对象中</p>
<p><code>etree.HTML(&#39;page_text&#39;)</code></p>
</li>
<li><p>3.<code>xpath(&#39;xpath表达式&#39;)</code></p>
</li>
</ul>
<p>xpath表达式</p>
<ul>
<li>&#x2F;	表示的是从根节点开始定位。表示的是一个层级</li>
<li>&#x2F;&#x2F;    表示的是多个层级。可以表示从任意位置开始定位</li>
<li>属性定位：&#x2F;&#x2F;div[@class&#x3D;”song]</li>
<li>索引定位：&#x2F;&#x2F;div[@class&#x3D;”song”]&#x2F;p[3，索引是从1开始的</li>
<li>获取属性：&#x2F;&#x2F;img&#x2F;@href</li>
</ul>
<h4 id="4-4-2-xpath实战"><a href="#4-4-2-xpath实战" class="headerlink" title="4.4.2.xpath实战"></a>4.4.2.xpath实战</h4><h5 id="4-4-2-1-xpath-58二手房"><a href="#4-4-2-1-xpath-58二手房" class="headerlink" title="4.4.2.1.xpath-58二手房"></a>4.4.2.1.xpath-58二手房</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ershoufang</span>():    response = requests.get(url=START_URL[<span class="string">&#x27;ershoufang&#x27;</span>],headers=HEADERS)    response_text = response.text    tree = etree.HTML(response_text)    <span class="comment"># txt_local_storage(response)    div_list = tree.xpath(&#x27;//*[@id=&quot;__layout&quot;]/div/section/section[3]/section[1]/section[2]/div&#x27;)    for div in div_list:        title = div.xpath(&#x27;.//div[@class=&quot;property-content-title&quot;]/h3/text()&#x27;)[0]        print(title)        ershoufang()</span></span><br></pre></td></tr></table></figure>

<h5 id="4-4-2-2-xpath-4k图片解析下载"><a href="#4-4-2-2-xpath-4k图片解析下载" class="headerlink" title="4.4.2.2.xpath-4k图片解析下载"></a>4.4.2.2.xpath-4k图片解析下载</h5><p>解析出的字段为乱码解决方案</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1response.encoding = &#x27;utf-8# 2image_name.encode(&#x27;iso-8859-01&#x27;).decode(&#x27;gbk&#x27;)# 3response = response.text.encode(&#x27;ISO-8859-1&#x27;)</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">meitu</span>():    my_mkdir(<span class="string">&#x27;./meitu&#x27;</span>)    response = requests.get(url=START_URL[<span class="string">&#x27;img_02&#x27;</span>],headers=HEADERS)    response_text = response.text    tree = etree.HTML(response_text)    li_list = tree.xpath(<span class="string">&#x27;//ul[@class=&quot;clearfix&quot;]/li&#x27;</span>)    detail_url_list = []    <span class="keyword">for</span> li <span class="keyword">in</span> li_list:        detail_url = li.xpath(<span class="string">&#x27;./a/@href&#x27;</span>)[<span class="number">0</span>]        detail_url_list.append(<span class="string">&#x27;https://pic.netbian.com&#x27;</span> + detail_url)    <span class="built_in">print</span>(detail_url_list)    <span class="keyword">for</span> detail_ele <span class="keyword">in</span> detail_url_list:        response_detail = requests.get(url=detail_ele,headers=HEADERS)        response_detail_text = response_detail.text.encode(<span class="string">&#x27;ISO-8859-1&#x27;</span>)        tree_detail = etree.HTML(response_detail_text)        img_src = tree_detail.xpath(<span class="string">&#x27;//div[@class=&quot;photo-pic&quot;]/a/img/@src&#x27;</span>)[<span class="number">0</span>]        img_src = <span class="string">&#x27;https://pic.netbian.com&#x27;</span> + img_src        img_response = requests.get(url=img_src,headers=HEADERS)        img_name = tree_detail.xpath(<span class="string">&#x27;//div[@class=&quot;photo-pic&quot;]/a/img/@title&#x27;</span>)[<span class="number">0</span>] + <span class="string">&#x27;.jpg&#x27;</span>        filepath = <span class="string">&#x27;./meitu/&#x27;</span> + img_name        content_local_storage(img_response,filepath)        <span class="built_in">print</span>(img_name+<span class="string">&#x27;下载完成&#x27;</span>)        time.sleep(<span class="number">1</span>)meitu() </span><br></pre></td></tr></table></figure>

<h5 id="4-4-2-3-xpath-全国城市名称爬取"><a href="#4-4-2-3-xpath-全国城市名称爬取" class="headerlink" title="4.4.2.3.xpath-全国城市名称爬取"></a>4.4.2.3.xpath-全国城市名称爬取</h5><p>xpath中，多个选择语句，可以用<code>|</code>符号连接</p>
<h5 id="4-4-2-4-作业"><a href="#4-4-2-4-作业" class="headerlink" title="4.4.2.4.作业"></a>4.4.2.4.作业</h5><p>爬取站长素材的简历模板</p>
<h2 id="5-验证码识别"><a href="#5-验证码识别" class="headerlink" title="5.验证码识别"></a>5.验证码识别</h2><h3 id="5-1-验证码识别简介"><a href="#5-1-验证码识别简介" class="headerlink" title="5.1.验证码识别简介"></a>5.1.验证码识别简介</h3><p>反爬机制：验证码，识别验证码图片中的数据，用于模拟登录操作。</p>
<p>识别验证码的操作：</p>
<ul>
<li>人工肉眼识别（不操作）</li>
<li>第三方自动识别</li>
</ul>
<h3 id="5-2-云打码使用流程"><a href="#5-2-云打码使用流程" class="headerlink" title="5.2.云打码使用流程"></a>5.2.云打码使用流程</h3><h4 id="5-2-1-云打码平台"><a href="#5-2-1-云打码平台" class="headerlink" title="5.2.1.云打码平台"></a>5.2.1.云打码平台</h4><p><a target="_blank" rel="noopener" href="http://www.ttshitu.com/docs/index.html?spm=null">http://www.ttshitu.com/docs/index.html?spm=null</a></p>
<h4 id="5-2-2-Tesseract-OCR完成验证码训练"><a href="#5-2-2-Tesseract-OCR完成验证码训练" class="headerlink" title="5.2.2.Tesseract-OCR完成验证码训练"></a>5.2.2.Tesseract-OCR完成验证码训练</h4><p>（时间成本、学习成本过高，略过）</p>
<h3 id="5-3-古诗文网验证码识别"><a href="#5-3-古诗文网验证码识别" class="headerlink" title="5.3.古诗文网验证码识别"></a>5.3.古诗文网验证码识别</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">base64_api</span>(<span class="params">uname, pwd, img, typeid</span>):    <span class="keyword">with</span> <span class="built_in">open</span>(img, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:        base64_data = base64.b64encode(f.read())        b64 = base64_data.decode()    data = &#123;<span class="string">&quot;username&quot;</span>: uname, <span class="string">&quot;password&quot;</span>: pwd, <span class="string">&quot;typeid&quot;</span>: typeid, <span class="string">&quot;image&quot;</span>: b64&#125;    result = json.loads(requests.post(<span class="string">&quot;http://api.ttshitu.com/predict&quot;</span>, json=data).text)    <span class="keyword">if</span> result[<span class="string">&#x27;success&#x27;</span>]:        <span class="keyword">return</span> result[<span class="string">&quot;data&quot;</span>][<span class="string">&quot;result&quot;</span>]    <span class="keyword">else</span>:        <span class="keyword">return</span> result[<span class="string">&quot;message&quot;</span>]    <span class="keyword">return</span> <span class="string">&quot;&quot;</span><span class="keyword">def</span> <span class="title function_">parse_ocr</span>(<span class="params">img_path,uname,pwd</span>):    result = base64_api(uname, pwd, img=img_path, typeid=<span class="number">3</span>)    <span class="built_in">print</span>(result)    <span class="keyword">def</span> <span class="title function_">query_account_nfo</span>(<span class="params">username, password</span>):    url = <span class="string">&#x27;http://api.ttshitu.com/queryAccountInfo.json?username=&#x27;</span>+username+<span class="string">&#x27;&amp;password=&#x27;</span>+password    result = requests.get(url=url,headers=HEADERS).text    <span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<h2 id="6-模拟登陆"><a href="#6-模拟登陆" class="headerlink" title="6.模拟登陆"></a>6.模拟登陆</h2><h3 id="6-1-模拟登陆实现流程梳理"><a href="#6-1-模拟登陆实现流程梳理" class="headerlink" title="6.1.模拟登陆实现流程梳理"></a>6.1.模拟登陆实现流程梳理</h3><p>爬取基于某些用户的用户信息</p>
<p>需求：模拟登录</p>
<ul>
<li>点击登录之后，会发起一个post请求</li>
<li>post请求中会携带登录之前录入的相关的登录信息（用户名、密码、验证码…）</li>
<li>验证码：每次请求都会变化</li>
</ul>
<h3 id="6-2模拟登陆"><a href="#6-2模拟登陆" class="headerlink" title="6.2模拟登陆"></a>6.2模拟登陆</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gsw</span>():    response_index = requests.get(url=START_URL[<span class="string">&#x27;gsw_index&#x27;</span>],headers=HEADERS).text    tree = etree.HTML(response_index)    img_src = tree.xpath(<span class="string">&#x27;//img[@id=&quot;imgCode&quot;]/@src&#x27;</span>)[<span class="number">0</span>]    img_src = <span class="string">&#x27;https://so.gushiwen.cn&#x27;</span> + img_src    response_code = requests.get(url=img_src,headers=HEADERS).content    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;code.jpg&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:        fw.write(response_code)    code_parse= parse_ocr(<span class="string">&#x27;./code.jpg&#x27;</span>,OCR_USERNAME,OCR_PASSWORD)    data = &#123;        <span class="string">&quot;__VIEWSTATE&quot;</span>: <span class="string">&quot;qehW4Za32V5Uek9jdmRmASRMZhBh28aebNG9fOHgl5F5cv4MfUrmJcSloFlvQV6QRmbmpt7oAiql3jOVGoYwEN88jjGokFQcptQt9NeH3BwYJn6MZe5PGbBE05c=&quot;</span>,        <span class="string">&quot;__VIEWSTATEGENERATOR&quot;</span>: <span class="string">&quot;C93BE1AE&quot;</span>,        <span class="string">&quot;from&quot;</span>: <span class="string">&quot;http&quot;</span>,        <span class="comment"># 账号密码自行注册        &quot;email&quot;: &quot;********&quot;,        &quot;pwd&quot;: &quot;********&quot;,                &quot;code&quot;: code_parse,        &quot;denglu&quot;: &quot;登录&quot;,    &#125;    response_login = requests.post(url=START_URL[&#x27;gsw_login&#x27;],data=data,headers=HEADERS)    print(response_login.status_code)    txt_local_storage(response_login,&#x27;login.html&#x27;)gsw()</span></span><br></pre></td></tr></table></figure>



<h3 id="6-3-模拟登陆cookie操作"><a href="#6-3-模拟登陆cookie操作" class="headerlink" title="6.3.模拟登陆cookie操作"></a>6.3.模拟登陆cookie操作</h3><p>需求：爬取当前用户的相关的用户信息（个人主页中显示的信息）</p>
<p>http&#x2F;https协议特性：无状态</p>
<p>没有请求到对应页面数据的原因：</p>
<ul>
<li>发起的第二次基于个人主页页面请求的时候，服务器端并不知道该次请求时基于登录状态下的请求。</li>
</ul>
<p>cookie：用来让服务器端记录客户端的相关状态</p>
<ul>
<li><p>手动处理：通过抓包工具获取cookie的值，将该值封装到headers中（不建议）</p>
</li>
<li><p>自动处理：</p>
<ul>
<li>cookie的值来源是哪里？<ul>
<li>模拟登录post请求后，由服务器端创建</li>
</ul>
</li>
<li>session会话对象：<ul>
<li>作用：<ul>
<li>1.可以进行请求的发送</li>
<li>如果请求过程中产生了cookie，则该cookie会被自动存储在该session对象中</li>
</ul>
</li>
</ul>
</li>
<li>创建一个session对象：<ul>
<li><code>session = requests.Session()</code></li>
</ul>
</li>
<li>使用session对象进行模拟登录post请求的发送（cookie就会被存储在session对象中）</li>
<li>session对象对个人主页的get请求进行发送（携带了cookie）</li>
</ul>
</li>
</ul>
<h2 id="7-代理"><a href="#7-代理" class="headerlink" title="7.代理"></a>7.代理</h2><h3 id="7-1-代理理论讲解"><a href="#7-1-代理理论讲解" class="headerlink" title="7.1.代理理论讲解"></a>7.1.代理理论讲解</h3><p>代理：破解封IP这种反爬机制</p>
<p>什么是代理：</p>
<ul>
<li>代理服务器</li>
</ul>
<p>代理的作用：</p>
<ul>
<li>突破自身IP访问的限制</li>
<li>隐藏自身真是IP</li>
</ul>
<p>代理相关的网站：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.taiyanghttp.com/free/">http://www.taiyanghttp.com/free/</a></li>
</ul>
<h3 id="7-2-代理在爬虫中的应用"><a href="#7-2-代理在爬虫中的应用" class="headerlink" title="7.2.代理在爬虫中的应用"></a>7.2.代理在爬虫中的应用</h3><p>代理ip的类型：</p>
<ul>
<li>http：应用到http协议对应的url中</li>
<li>https：应用到https协议对应的url中</li>
</ul>
<p>代理ip的匿名度：</p>
<ul>
<li>透明，服务器知道该次请求使用了代理，也知道请求对应的真是ip</li>
<li>匿名，知道使用了代理，不知道真是ip</li>
<li>高匿，不知道使用了代理，更不知道真实的ip</li>
</ul>
<p>代码： </p>
<p><code>response = requests.get(url=url,headers=headers,proxies=&#123;&quot;http&quot;:&quot;1.127.0.1:8080&#125;)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests<span class="comment"># 根据协议类型，选择不同的代理proxies = &#123;  &quot;http&quot;: &quot;http://12.34.56.79:9527&quot;,  &quot;https&quot;: &quot;http://12.34.56.79:9527&quot;,&#125;response = requests.get(&quot;http://www.baidu.com&quot;, proxies = proxies)print response.text</span></span><br></pre></td></tr></table></figure>



<h2 id="8-异步爬虫"><a href="#8-异步爬虫" class="headerlink" title="8.异步爬虫"></a>8.异步爬虫</h2><h3 id="8-1-异步爬虫概述"><a href="#8-1-异步爬虫概述" class="headerlink" title="8.1.异步爬虫概述"></a>8.1.异步爬虫概述</h3><p>在爬虫中使用异步，实现高性能的数据爬取操作</p>
<p>异步爬虫的方式：</p>
<ul>
<li>多线程，多进程：<ul>
<li>好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。</li>
<li>弊端：无法无限制的开启多线程或者多进程</li>
</ul>
</li>
</ul>
<h3 id="8-2-异步爬虫-多进程-amp-多线程"><a href="#8-2-异步爬虫-多进程-amp-多线程" class="headerlink" title="8.2.异步爬虫-多进程&amp;多线程"></a>8.2.异步爬虫-多进程&amp;多线程</h3><ul>
<li>进程池、线程池（适当地使用）<ul>
<li>可以降低系统对进程和线程创建和销毁的一个频率，从而很好的降低系统的开销。</li>
<li>弊端：池中线程的数量是有上限的。</li>
</ul>
</li>
</ul>
<h3 id="8-3-异步爬虫-进程池-amp-线程池"><a href="#8-3-异步爬虫-进程池-amp-线程池" class="headerlink" title="8.3.异步爬虫-进程池&amp;线程池"></a>8.3.异步爬虫-进程池&amp;线程池</h3><p><strong>单线程模拟</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params"><span class="built_in">str</span></span>):    <span class="built_in">print</span>(<span class="string">&#x27;downloading...&#x27;</span>)    time.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;don=wnloaded successed!&#x27;</span>,<span class="built_in">str</span>)name_list= [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>]strat_time = time.time()<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="built_in">len</span>(name_list)):    get_page(name_list[i])end_time = time.time()<span class="built_in">print</span>(<span class="string">&#x27;%d seconds&#x27;</span> % (end_time - strat_time))</span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downloading...donwnloaded successed! adownloading...donwnloaded successed! bdownloading...donwnloaded successed! cdownloading...donwnloaded successed! d8 seconds</span><br></pre></td></tr></table></figure>

<p><strong>线程池</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_page</span>(<span class="params"><span class="built_in">str</span></span>):    <span class="built_in">print</span>(<span class="string">&#x27;downloading...&#x27;</span>)    time.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;donwnloaded successed!&#x27;</span>,<span class="built_in">str</span>)name_list= [<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>]strat_time = time.time()<span class="comment"># 实例化一个线程池对象pool = Pool(4)# 将列表的每一个列表元素传递给get_page依次处理pool.map(get_page, name_list)end_time = time.time()print(&#x27;%d seconds&#x27; % (end_time - strat_time))# 关闭线程池pool.close()pool.join()</span></span><br></pre></td></tr></table></figure>

<p>结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">downloading...downloading...downloading...downloading...donwnloaded successed! bdonwnloaded successed! adonwnloaded successed! cdonwnloaded successed! d2 seconds</span><br></pre></td></tr></table></figure>



<h3 id="8-4-异步爬虫-线程池案例应用"><a href="#8-4-异步爬虫-线程池案例应用" class="headerlink" title="8.4.异步爬虫-线程池案例应用"></a>8.4.异步爬虫-线程池案例应用</h3><p><a target="_blank" rel="noopener" href="https://www.pearvideo.com/category_5">https://www.pearvideo.com/category_5</a></p>
<p>该网站视频的src链接，是放在ajax的相应对象中的，其对应的url有两个参数，其中一个参数是动态加载的，需要用selenium</p>
<h3 id="9-1-协程相关概念"><a href="#9-1-协程相关概念" class="headerlink" title="9.1.协程相关概念"></a>9.1.协程相关概念</h3><h4 id="9-1-1-异步编程"><a href="#9-1-1-异步编程" class="headerlink" title="9.1.1.异步编程"></a>9.1.1.异步编程</h4><p>为什么要讲？</p>
<ul>
<li>异步非阻塞、asyncio等等概念，或多或少的听说过</li>
<li>tornado、fastapi、django3.x asgi、aiohttp等框架或者模块中，都在使用异步（提升性能）</li>
</ul>
<p>如何讲解？</p>
<ul>
<li>asyncio模块进行异步编程</li>
<li>实战案例</li>
</ul>
<h4 id="9-1-2-协程"><a href="#9-1-2-协程" class="headerlink" title="9.1.2.协程"></a>9.1.2.协程</h4><p>协程不是计算机提供，而是程序员<strong>人为创造</strong>的</p>
<p>协程（Coroutine），也可以被称为微线程、是一种用户态的上下文切换技术</p>
<p>简而言之，其实就是通过一个线程实现代码块（不同函数之间）<strong>相互切换</strong>执行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">func1</span>():    <span class="built_in">print</span>(<span class="string">&#x27;1&#x27;</span>)    ...    <span class="built_in">print</span>(<span class="string">&#x27;2n-1&#x27;</span>)    <span class="keyword">def</span> <span class="title function_">func2</span>():    <span class="built_in">print</span>(<span class="string">&#x27;2&#x27;</span>)    ...    <span class="built_in">print</span>(<span class="string">&#x27;2n&#x27;</span>)    func1()func2()</span><br></pre></td></tr></table></figure>

<p>如上两个函数，协程可以让函数中的不同语句交错执行</p>
<p>实现协程的方法：</p>
<ul>
<li><code>greenlet</code>，比较早期的模块。</li>
<li><code>yield关键字</code></li>
<li><code>asyncio装饰器</code><ul>
<li>python3.4引入的</li>
</ul>
</li>
<li><code>async/await关键字</code><ul>
<li>python3.5引入的</li>
<li>目前比较主流</li>
</ul>
</li>
</ul>
<h5 id="9-1-2-1-greenlet实现协程"><a href="#9-1-2-1-greenlet实现协程" class="headerlink" title="9.1.2.1.greenlet实现协程"></a>9.1.2.1.greenlet实现协程</h5><p>安装<code>pip install greenlet</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-from greenlet import greenletdef func1():    print(&#x27;1&#x27;) # 第2步，输出1    gr2.switch() # 第3步，切换到func2函数    print(&#x27;2&#x27;) # 第6步，输出2    gr2.switch() # 第7步，切换到func2函数，从上一次执行的位置继续向后执行 def func2():    print(&#x27;3&#x27;) # 第4步，输出3    gr1.switch() # 第5步，切换到func1函数，从上一次的位置继续向后执行    print(&#x27;4&#x27;) # 第8步，输出4    gr1 = greenlet(func1)gr2 = greenlet(func2)gr1.switch() # 第1步，去执行func1</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-2-2-yield关键字"><a href="#9-1-2-2-yield关键字" class="headerlink" title="9.1.2.2.yield关键字"></a>9.1.2.2.yield关键字</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-def func1():    yield 1    yield from func2()    yield 2    def func2():    yield 3    yield 4    f1 = func1()for item in f1:    print(item)</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-2-3-asyncio"><a href="#9-1-2-3-asyncio" class="headerlink" title="9.1.2.3.asyncio"></a>9.1.2.3.asyncio</h5><p>在python3.4以及之后的版本</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-import asyncio@asyncio.coroutinedef func1():    print(&#x27;1&#x27;)    yield from asyncio.sleep(2) # 遇到IO耗时操作，自动切换到task中的其他任务    print(&#x27;2&#x27;)    @asyncio.coroutinedef func2():    print(&#x27;3&#x27;)    yield from asyncio.sleep(2) # 遇到IO耗时操作，自动切换到task中的其他任务    print(&#x27;4&#x27;)tasks = [        asyncio.ensure_future(func1()),        asyncio.ensure_future(func2())        ]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))</span></span><br></pre></td></tr></table></figure>

<p>注意：注意IO自动切换</p>
<h5 id="9-1-2-3-async-amp-await关键字"><a href="#9-1-2-3-async-amp-await关键字" class="headerlink" title="9.1.2.3.async &amp; await关键字"></a>9.1.2.3.async &amp; await关键字</h5><p>自python3.5及以后的版本</p>
<p>和上面的本质上类似，可以理解为语法糖</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-import asyncioasync def func1():    print(&#x27;1&#x27;)    await asyncio.sleep(2) # 遇到IO耗时操作，自动切换到task中的其他任务    print(&#x27;2&#x27;)    async def func2():    print(&#x27;3&#x27;)    await asyncio.sleep(2) # 遇到IO耗时操作，自动切换到task中的其他任务    print(&#x27;4&#x27;)tasks = [        asyncio.ensure_future(func1()),        asyncio.ensure_future(func2())        ]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))</span></span><br></pre></td></tr></table></figure>

<h4 id="9-1-3-协程意义"><a href="#9-1-3-协程意义" class="headerlink" title="9.1.3.协程意义"></a>9.1.3.协程意义</h4><p>在一个线程中如果遇到IO等待时间，线程不会傻傻等，利用空闲时间再去干点其他事</p>
<p>案例：去下载三站图片（网络IO）</p>
<ul>
<li><p>普通方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requestsheaders =&#123;    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span>&#125;<span class="keyword">def</span> <span class="title function_">download_images</span>(<span class="params">url</span>):    <span class="built_in">print</span>(<span class="string">&#x27;正在下载&#x27;</span>,url)    response = requests.get(url=url,headers=headers).content    file_name = url.split(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]    <span class="keyword">with</span> <span class="built_in">open</span>(file_name,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:        fw.write(response)    <span class="built_in">print</span>(<span class="string">&#x27;下载结束&#x27;</span>,url)urls = [    <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210812/230003-1628780403b213.jpg&quot;</span>,    <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210718/001826-16265387066216.jpg&quot;</span>,    <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210812/225733-16287802533d30.jpg&quot;</span>,]<span class="keyword">for</span> url <span class="keyword">in</span> urls:    download_images(url)</span><br></pre></td></tr></table></figure>
</li>
<li><p>基于协程的异步</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttpimport asyncioasync <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">session, url</span>):    <span class="built_in">print</span>(<span class="string">&#x27;发送请求&#x27;</span>,url)    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url,verify_ssl=<span class="literal">False</span>) <span class="keyword">as</span> response:        content = <span class="keyword">await</span> response.content.read()        filename = url.rsplit(<span class="string">&#x27;/&#x27;</span>)[-<span class="number">1</span>]        <span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:            fw.write(content)<span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:        urls = [            <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210812/230003-1628780403b213.jpg&quot;</span>,            <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210718/001826-16265387066216.jpg&quot;</span>,            <span class="string">&quot;https://pic.netbian.com/uploads/allimg/210812/225733-16287802533d30.jpg&quot;</span>,        ]        tasks = [asyncio.create_task(fetch(session,url)) <span class="keyword">for</span> url <span class="keyword">in</span> urls]        <span class="keyword">await</span> asyncio.wait(tasks)<span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:    asyncio.run(main())</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="9-1-4-异步编程"><a href="#9-1-4-异步编程" class="headerlink" title="9.1.4.异步编程"></a>9.1.4.异步编程</h4><h5 id="9-1-4-1asyncio事件循环"><a href="#9-1-4-1asyncio事件循环" class="headerlink" title="9.1.4.1asyncio事件循环"></a>9.1.4.1asyncio事件循环</h5><p>理解为一个死循环，去检测并执行某些代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># 伪代码任务列表 = [任务1， 任务2， 任务3...]while True:	可执行的任务列表， 已完成的任务列表 = 去任务列表中检查所有的任务，将&#x27;可执行&#x27;和&#x27;已完成&#x27;的任务返回		for 就绪任务 in 可执行的任务列表：		在任务列表中移除 已完成的任务			如果 任务列表 中的任务都已完成，则终止循环</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio<span class="comment"># 生成（获取）一个事件循环loop = asyncio.get_event_loop()# 将任务放到&#x27;任务列表&#x27;loop.run_until_complete(任务) </span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-2-快速上手"><a href="#9-1-4-2-快速上手" class="headerlink" title="9.1.4.2.快速上手"></a>9.1.4.2.快速上手</h5><p>协程函数：定义函数的时候，加上修饰符<code>async</code></p>
<p>协程对象：执行协程函数得到的协程对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">func</span>():    passresult = func()</span><br></pre></td></tr></table></figure>

<p>注意：执行协程函数创建爱你协程对象，函数内部代码不会执行</p>
<p>如果想要运行协程函数内部代码，必须要将协程对象交给事件循环来处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&#x27;aa&#x27;</span>)   result = func()<span class="comment"># loop = async.get_event_loop()# loop.run_untl_complete(result)asyncio.run(result) # python3.7</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-3-await关键词"><a href="#9-1-4-3-await关键词" class="headerlink" title="9.1.4.3.await关键词"></a>9.1.4.3.await关键词</h5><p>await + 可等待对象（协程对象、Future、Task对象），如IO等待</p>
<p>示例一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&#x27;aa&#x27;</span>)    response = <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="comment"># 只有等待结束，有结果了，才会继续向下执行    print(&#x27;end&#x27;,response)    asyncio.run(func())</span></span><br></pre></td></tr></table></figure>

<p>示例二：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">others</span>():    <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>)    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;end&#x27;</span>)    <span class="keyword">return</span> <span class="string">&#x27;返回值&#x27;</span><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&quot;执行协程函数内部代码&quot;</span>)    	<span class="comment"># 遇到IO操作挂起当前协程（任务），等IO操作完成之后，再继续往下执行，当前协程对象挂起时，事件循环可以去执行其他协程（任务）    response = await others()        print(&#x27;IO请求结束，结果为：&#x27;,response)        asyncio.run(func())</span></span><br></pre></td></tr></table></figure>

<p>示例三：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">others</span>():    <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>)    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;end&#x27;</span>)    <span class="keyword">return</span> <span class="string">&#x27;返回值&#x27;</span><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&quot;执行协程函数内部代码&quot;</span>)    	<span class="comment"># 遇到IO操作挂起当前协程（任务），等IO操作完成之后，再继续往下执行，当前协程对象挂起时，事件循环可以去执行其他协程（任务）    response1 = await others()        print(&#x27;IO请求结束，结果为：&#x27;,response1)        response2 = await others()        print(&#x27;IO请求结束，结果为：&#x27;,response2)    asyncio.run(func())</span></span><br></pre></td></tr></table></figure>

<p>await就是等待对象的值得到结果之后，再继续向下走</p>
<h5 id="9-1-4-4-task对象"><a href="#9-1-4-4-task对象" class="headerlink" title="9.1.4.4.task对象"></a>9.1.4.4.task对象</h5><p>Tasks are used to schedule coroutines concurrently.<br>When a coroutine is wrapped into a Task with functions like <code>asyncio.create_task()</code> the coroutine is automatically scheduled to the soon.</p>
<p>白话：在事件循环中添加多个任务的</p>
<p>Tasks用于并发调度协程，通过asynio.create_task(协程对象)的方式创建Task对象，这样可以让协程加入事件循环中等待被调度执行。除了使用<code>asyncio.create_task()</code>函数以外，还可以用更低层级的<code>loop.create_task()</code>和<code>ensure_future()</code>函数。不建议手动实例化Task对象。</p>
<p>注意：<code>asyncio.create_task()</code>函数在Python3.7中被加入。在Python3.7之前，可以改用低层级的<code>loop.create_task()</code>和<code>ensure_future()</code>函数。</p>
<p>示例一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&#x27;1&#x27;</span>)    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;2&#x27;</span>)    <span class="keyword">return</span> <span class="string">&#x27;返回值&#x27;</span><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="built_in">print</span>(<span class="string">&#x27;main start&#x27;</span>)        <span class="comment"># 创建Task对象，将当前执行func函数添加到事件循环    task1 = asyncio.create_task(func())    task2 = asyncio.create_task(func())        print(&#x27;main end&#x27;)        # 当执行某协程遇到IO操作时，会自动化切换执行其他任务    # 此处的await是等待相对应的协程，全部执行完毕后，然后获取结果    ret1 = await task1    ret2 = await task2        print(ret1, ret2)    asyncio.run(main())</span></span><br></pre></td></tr></table></figure>

<p>执行结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main startmain end1122返回值 返回值</span><br></pre></td></tr></table></figure>

<p>示例二：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&#x27;1&#x27;</span>)    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;2&#x27;</span>)    <span class="keyword">return</span> <span class="string">&#x27;返回值&#x27;</span><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="built_in">print</span>(<span class="string">&#x27;main start&#x27;</span>)        task_list=[        asyncio.create_task(func(),name=<span class="string">&#x27;n1&#x27;</span>),        asyncio.create_task(func(),name=<span class="string">&#x27;n2&#x27;</span>)    ]        <span class="built_in">print</span>(<span class="string">&#x27;main end&#x27;</span>)    <span class="comment"># done默认提供的是集合    # 如果timeout=1，执行时还没有完成，pending就是那个没有完成的东西    # 默认timeout=None，等待全部完成    done,pending = await asyncio.wait(task_list,timeout=None)    print(done)asyncio.run(main())</span></span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">main startmain end1122&#123;&lt;Task finished name=&#x27;n1&#x27; coro=&lt;func() done, defined at F:\workspace\test.py:4&gt; result=&#x27;返回值&#x27;&gt;, &lt;Task finished name=&#x27;n2&#x27; coro=&lt;func() done, defined at F:\workspace\test.py:4&gt; result=&#x27;返回值&#x27;&gt;&#125;</span><br></pre></td></tr></table></figure>

<p>示例三：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">func</span>():    <span class="built_in">print</span>(<span class="string">&#x27;1&#x27;</span>)    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    <span class="built_in">print</span>(<span class="string">&#x27;2&#x27;</span>)    <span class="keyword">return</span> <span class="string">&#x27;返回值&#x27;</span>task_list=[    func(),    func(),]done,pending = asyncio.run(asyncio.wait(task_list))<span class="built_in">print</span>(done)</span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-5-async的future对象"><a href="#9-1-4-5-async的future对象" class="headerlink" title="9.1.4.5.async的future对象"></a>9.1.4.5.async的future对象</h5><p>A <code>Future</code> is a special <code>low-level</code> awaitable object that represents an eventual result of an asynchronous operation.</p>
<p>Task继承Future，Task对象内部await结果的处理，基于Future对象来的。</p>
<p>示例一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="comment"># 获取当前事件循环    loop = asyncio.get_running_loop()        # 创建一个任务（future对象），这个任务什么都不干    fut = loop.create_future()        # 等待任务最终结束(Future)，没有结果会一直等下去    await fut</span></span><br></pre></td></tr></table></figure>

<p>示例二：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioasync <span class="keyword">def</span> <span class="title function_">set_after</span>(<span class="params">fut</span>):    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)    fut.set_result(<span class="string">&#x27;aaa&#x27;</span>)<span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="comment"># 获取当前事件循环    loop = asyncio.get_running_loop()    # 创建一个任务（Future）对象，没绑定任何行为，则这个任务永远不知道什么时候结束    fut = loop.create_future()    # 创建一个任务（Task对象），绑定了set_after函数，函数内在2秒之后，会给fut赋值    # 即手动设置future任务的最终结果，那么future就可以结束了    await loop.create_task(set_after(fut))    # 等待Future对象获取最终结果，否则一直等待下去    data = await fut    print(data)asyncio.run(main())</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-6-concurrent的future对象"><a href="#9-1-4-6-concurrent的future对象" class="headerlink" title="9.1.4.6.concurrent的future对象"></a>9.1.4.6.concurrent的future对象</h5><p><code>concurrent.futures.Future</code></p>
<p>使用线程池、进程池实现异步操作时用到的对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timefrom concurrent.futures <span class="keyword">import</span> Futurefrom concurrent.futures.thread <span class="keyword">import</span> ThreadPoolExecutorfrom concurrent.futures.process <span class="keyword">import</span> ProcessPoolExecutordef func(value):    time.sleep(<span class="number">1</span>)    <span class="built_in">print</span>(value)    <span class="keyword">return</span> <span class="number">123</span><span class="comment"># 创建线程池pool = ThreadPoolExecutor(max_workers=5)# 创建进程池# pool = ProcessPoolExecutor(max_workers=5)for i in range(10):    fut = pool.submit(func,i)    print(fut)    </span></span><br></pre></td></tr></table></figure>

<p>以后写代码，可能会存在交叉使用。</p>
<p>例如，crm项目80%都是基于协程和异步编程 + MySQL（不支持）【线程、进程做异步编程】</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> timeimport asyncioimport concurrent.futuresdef func1():    <span class="comment"># 某个耗时操作    time.sleep(2)    return 123async def main():    loop = asyncio.get_running_loop()    # 1. Run in the default loop&#x27;s executor(默认ThreadPoolExecutor)    # 第一步：内部会先调用 ThreadPoolExcutor的submit方法去线程池中申请一个线程去执行func函数，并返回一个concurrent.futures.Future对象    # 第二步，调用asyncio.wrap_future将concurrent.futures.Future对象包装为asyncio.Future对象    # 因为concurrent.futures.Future对象不支持await语法，所以需要包装为asyncio.Future才能使用    fut = loop.run_in_executor(None,func1)    result = await fut    print(&#x27;default thread pool&#x27;,result)    # 2.Run in a custom thread pool;    # with concurrent.futures.ThreadPoolExecutor() as pool:    #     result = await loop.run_in_executor(pool, func1)    #     print(&#x27;custom thread pool&#x27;,result)    # 3.Run in a custom process pool;    # with concurrent.futures.ProcessPoolExecutor() as pool:    #     result = await loop.run_in_executor(pool, func1)    #     print(&#x27;custom process pool&#x27;,result)asyncio.run(main())</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-7-异步和非异步混合案例"><a href="#9-1-4-7-异步和非异步混合案例" class="headerlink" title="9.1.4.7.异步和非异步混合案例"></a>9.1.4.7.异步和非异步混合案例</h5><p>案例，asyncio+不支持异步的模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requestsimport asyncioasync <span class="keyword">def</span> <span class="title function_">download_image</span>(<span class="params">url</span>):    <span class="comment"># 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动切换到其他任务）    print(&#x27;download start&#x27;,url)    loop = asyncio.get_event_loop()    # reqeusts模块不支持异步操作，所以就使用线程池来配合实现了    future = loop.run_in_executor(None,requests.get,url)    resposne = await future    print(&#x27;download end&#x27;)    # 图片保存到本地    filename = url.split(&#x27;/&#x27;)[-1]    with open(filename,&#x27;wb&#x27;) as fw:        fw.write(resposne.content)if __name__ ==&#x27;__main__&#x27;:    url_list = [        &#x27;https://pic.netbian.com/uploads/allimg/210817/235554-162921575410ce.jpg&#x27;,        &#x27;https://pic.netbian.com/uploads/allimg/210816/234129-162912848931ba.jpg&#x27;,        &#x27;https://pic.netbian.com/uploads/allimg/210815/233459-16290416994668.jpg&#x27;,    ]    tasks = [download_image(url) for url in url_list]    loop = asyncio.get_event_loop()    loop.run_until_complete(asyncio.wait(tasks))</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-8-异步迭代器"><a href="#9-1-4-8-异步迭代器" class="headerlink" title="9.1.4.8.异步迭代器"></a>9.1.4.8.异步迭代器</h5><p>什么是异步迭代器</p>
<p>实现饿了<code>__aiter__()</code>和<code>__anext__()</code>方法的对象，<code>__anext__()</code>必须返回一个<code>awaitable</code>对象，<code>async_for</code>会处理异步迭代器的<code>__anext()__</code>方法所返回的可等待都对象，直到其引发一个<code>StopAsyncIteration</code>异常。</p>
<p>什么时异步可迭代对象</p>
<p>可在<code>async_for</code>语句中被使用的对象，必须通过它的<code>__aiter__()</code>方法返回一个<code>asynchronous iterator</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioclass Reader(<span class="built_in">object</span>):	<span class="comment"># &quot;&quot;&quot;自定义异步迭代器（同时也是异步可迭代对象）&quot;&quot;&quot;        def __init__(self):        self.count = 0            async def readline(self):        # await asyncio.sleep(2)        self.count += 1        if self.count == 100:            return None        return self.count        def __aiter__(self):        return self        async def __anext__(self):        val = await self.readline()        if val == None:            raise StopAsyncIteration        return valasync def func():    obj = Reader()    async for item in obj:        print(item)                asyncio.run(func())</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-9-异步上下文管理器"><a href="#9-1-4-9-异步上下文管理器" class="headerlink" title="9.1.4.9.异步上下文管理器"></a>9.1.4.9.异步上下文管理器</h5><p>此种对象通过定义<code>__aenter__()</code>和<code>aexit__()</code>方法来对<code>async with</code>语句中的环境进行控制。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioclass AsyncContextMannager:    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):        self.conn = conn    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">do_something</span>(<span class="params">self</span>):        <span class="comment"># 异步操作数据库        return 123        async def __aenter__(self):        # 异步链接数据库        # self.conn = await asyncio.sleep(1)        return self        async def __aexit__(self,exc_type,tb):        # 异步关闭数据库链接        await asyncio.sleep(1)async def func():    async with AsyncContextMannager() as f:        resutl  = await f.do_something()        print(resutl)asyncio.run(func())</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-10-uvloop"><a href="#9-1-4-10-uvloop" class="headerlink" title="9.1.4.10.uvloop"></a>9.1.4.10.uvloop</h5><p>是asyncio的事件循环的替代方案。事件循环 &gt; 默认asyncio的事件循环</p>
<p>运行速度堪比go</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uvloop</span><br></pre></td></tr></table></figure>

<p>注意：不支持windows</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioimport uvloopasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())<span class="comment"># 编写asyncio的代码，与之前的代码一致# 内部的事件循环，会由uvloop替代asyncio.run(...)</span></span><br></pre></td></tr></table></figure>

<p>asgi中的uvcorn，使用的就是uvloop</p>
<h5 id="9-1-4-11-案例-异步操作redis"><a href="#9-1-4-11-案例-异步操作redis" class="headerlink" title="9.1.4.11.案例-异步操作redis"></a>9.1.4.11.案例-异步操作redis</h5><p>在使用python操作redis时，链接&#x2F;操作&#x2F;断开都是网络IO</p>
<p><code>pip install aioredis</code></p>
<p>示例一：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioimport aioredisasync <span class="keyword">def</span> <span class="title function_">execute</span>(<span class="params">address,passward</span>):    <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>,address)    redis = <span class="keyword">await</span> aioredis.create_redis(address,passward=passward)    <span class="comment"># 网络IO操作    result = await redis.hmset_dict(&#x27;car&#x27;, key1=1, key2=2, key3=3)    print(result)    redis.close()    # 网络IO操作，关闭redis链接    await redis.wait_closed()    print(&#x27;end&#x27;,address)asyncio.run(execute(&#x27;redis://127.0.0.2:6379&#x27;,&#x27;123&#x27;))</span></span><br></pre></td></tr></table></figure>

<p>示例二：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncioimport aioredisasync <span class="keyword">def</span> <span class="title function_">execute</span>(<span class="params">address,passward</span>):    <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>,address)    redis = <span class="keyword">await</span> aioredis.create_redis(address,passward=passward)    <span class="comment"># 网络IO操作    result = await redis.hmset_dict(&#x27;car&#x27;, key1=1, key2=2, key3=3)    print(result)    redis.close()    # 网络IO操作，关闭redis链接    await redis.wait_closed()    print(&#x27;end&#x27;,address)task_list  = [    execute(&#x27;redis://127.0.0.1:6379&#x27;,&#x27;123&#x27;),    execute(&#x27;redis://127.0.0.2:6379&#x27;,&#x27;123&#x27;),]asyncio.run(asyncio.wait(task_list))</span></span><br></pre></td></tr></table></figure>

<h5 id="9-1-4-12-案例-异步操作mysql"><a href="#9-1-4-12-案例-异步操作mysql" class="headerlink" title="9.1.4.12.案例-异步操作mysql"></a>9.1.4.12.案例-异步操作mysql</h5><h5 id="9-1-4-13-FastApi框架异步"><a href="#9-1-4-13-FastApi框架异步" class="headerlink" title="9.1.4.13.FastApi框架异步"></a>9.1.4.13.FastApi框架异步</h5><h5 id="9-1-4-14-异步爬虫"><a href="#9-1-4-14-异步爬虫" class="headerlink" title="9.1.4.14.异步爬虫"></a>9.1.4.14.异步爬虫</h5><p><code>pip install aiohttp</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> aiohttpimport asyncioasync <span class="keyword">def</span> <span class="title function_">fetch</span>(<span class="params">session, url</span>):    <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>, url)    <span class="keyword">async</span> <span class="keyword">with</span> session.get(url, verify_ssl = <span class="literal">False</span>) <span class="keyword">as</span> response:        text = <span class="keyword">await</span> response.text()        <span class="built_in">print</span>(<span class="string">&#x27;result：&#x27;</span>, url, <span class="built_in">len</span>(text))        <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">main</span>():    <span class="keyword">async</span> <span class="keyword">with</span> aiohttp.ClientSession() <span class="keyword">as</span> session:        url_list =[            <span class="string">&#x27;https://www.baidu.com&#x27;</span>,            <span class="string">&#x27;https://www.qq.com&#x27;</span>,            <span class="string">&#x27;https://pic.netbian.com/4kmeinv/&#x27;</span>        ]        tasks = [asyncio.create_task(fetch(session, url)) <span class="keyword">for</span> url <span class="keyword">in</span> url_list]        done, pending = <span class="keyword">await</span> asyncio.wait(tasks)<span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:    asyncio.run(main())</span><br></pre></td></tr></table></figure>



<h5 id="9-1-4-15-总结"><a href="#9-1-4-15-总结" class="headerlink" title="9.1.4.15.总结"></a>9.1.4.15.总结</h5><h3 id="9-2-协程相关操作"><a href="#9-2-协程相关操作" class="headerlink" title="9.2.协程相关操作"></a>9.2.协程相关操作</h3><h3 id="9-3-多任务异步协程，实现异步爬虫"><a href="#9-3-多任务异步协程，实现异步爬虫" class="headerlink" title="9.3.多任务异步协程，实现异步爬虫"></a>9.3.多任务异步协程，实现异步爬虫</h3><h2 id="10-aiohttp模块"><a href="#10-aiohttp模块" class="headerlink" title="10.aiohttp模块"></a>10.aiohttp模块</h2><h3 id="10-1-aiohttp模块引出"><a href="#10-1-aiohttp模块引出" class="headerlink" title="10.1.aiohttp模块引出"></a>10.1.aiohttp模块引出</h3><h3 id="10-2-aiohttp-多任务异步协程，实现异步爬虫"><a href="#10-2-aiohttp-多任务异步协程，实现异步爬虫" class="headerlink" title="10.2.aiohttp+多任务异步协程，实现异步爬虫"></a>10.2.aiohttp+多任务异步协程，实现异步爬虫</h3><h2 id="11-selenium"><a href="#11-selenium" class="headerlink" title="11.selenium"></a>11.selenium</h2><h3 id="11-1-selenium简介"><a href="#11-1-selenium简介" class="headerlink" title="11.1.selenium简介"></a>11.1.selenium简介</h3><p>selenium模块和爬虫之间有怎样的关联？</p>
<ul>
<li>便捷的获取网站中动态加载的数据</li>
<li>便捷实现模拟登录</li>
</ul>
<p>什么是selenium模块？</p>
<ul>
<li>基于浏览器自动化的一个模块</li>
</ul>
<h3 id="11-2-selenium初试"><a href="#11-2-selenium初试" class="headerlink" title="11.2.selenium初试"></a>11.2.selenium初试</h3><p>selenium使用流程</p>
<ul>
<li><p>环境安装：<code>pip install selenium</code></p>
</li>
<li><p>下载浏览器的驱动程序</p>
<ul>
<li>下载链接：<a target="_blank" rel="noopener" href="http://npm.taobao.org/mirrors/chromedriver/">http://npm.taobao.org/mirrors/chromedriver/</a></li>
<li>驱动程序和浏览器的对应关系：<a target="_blank" rel="noopener" href="https://blog.csdn.net/huilan_same/article/details/51896672">https://blog.csdn.net/huilan_same/article/details/51896672</a></li>
</ul>
</li>
<li><p>编写基于浏览器自动化的操作代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriverfrom lxml <span class="keyword">import</span> etreefrom time <span class="keyword">import</span> sleep<span class="comment"># 实例化一个浏览器对象（传入浏览器的驱动程序）bro = webdriver.Chrome(executable_path = &#x27;./chromedriver.exe&#x27;)# 让浏览器发起一个指定url对应请求bro.get(&#x27;http://npm.taobao.org/mirrors/chromedriver/92.0.4515.107/&#x27;)# 获取浏览器当前页面的源码数据page_text = bro.page_source# 解析字段tree = etree.HTML(page_text)src = tree.xpath(&#x27;//div[@class=&quot;container&quot;]/pre/a/@href&#x27;)for i in range(0,len(src)):    print(src[i])sleep(5)bro.quit()</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>注意点：谷歌中文官网上的浏览器，默认安装在c盘，安装之后不要移动目录，否则驱动无法检测。</p>
</li>
</ul>
<h3 id="11-3-selenium其他自动化操作"><a href="#11-3-selenium其他自动化操作" class="headerlink" title="11.3.selenium其他自动化操作"></a>11.3.selenium其他自动化操作</h3><ul>
<li>编写基于浏览器自动化的操作代码<ul>
<li>发起请求：<code>get(url)</code></li>
<li>标签定位：find系列的方法</li>
<li>标签交互：<code>send_keys(&#39;xxx&#39;)</code></li>
<li>执行js程序：<code>excuted_script(&#39;jsCode&#39;)</code></li>
<li>前进、后退：<code>back()</code>、<code>forward()</code></li>
<li>关闭浏览器：<code>quit()</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriverfrom lxml <span class="keyword">import</span> etreefrom time <span class="keyword">import</span> sleepbro = webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)bro.get(<span class="string">&#x27;https://www.taobao.com&#x27;</span>)search_input = bro.find_element_by_id(<span class="string">&#x27;q&#x27;</span>)search_input.send_keys(<span class="string">&#x27;Iphoe&#x27;</span>)bro.execute_script(<span class="string">&#x27;window.scrollTo(0.document.body.scrollHeight)&#x27;</span>)sleep(<span class="number">2</span>)btn =bro.find_element_by_css_selector(<span class="string">&#x27;.btn-search&#x27;</span>)btn.click()bro.get(<span class="string">&#x27;https://www.baidu.com&#x27;</span>)sleep(<span class="number">2</span>)bro.back()sleep(<span class="number">5</span>)bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="11-4-iframe处理-动作链"><a href="#11-4-iframe处理-动作链" class="headerlink" title="11.4.iframe处理+动作链"></a>11.4.iframe处理+动作链</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriverfrom time <span class="keyword">import</span> sleep<span class="comment"># 导入动作链对应的类from selenium.webdriver import ActionChainsbro = webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;)bro.get(&#x27;https://www.runoob.com/try/try.php?filename=jqueryui-api-droppable&#x27;)# 如果定位的标签是存在ifram标签之中的，则必须通过如下操作，再进行标签定位bro.switch_to.frame(&#x27;iframeResult&#x27;) # 切换浏览器标签定位的作用域div = bro.find_element_by_id(&#x27;draggable&#x27;)# 动作链action = ActionChains(bro)# 点击长按指定的标签action.click_and_hold(div)for i in range(5):    # perform()表示立即执行动作链操作    action.move_by_offset(7,0).perform()    sleep(0.3)# 释放动作链action.release()print(div)</span></span><br></pre></td></tr></table></figure>

<h3 id="11-5-selenium的模拟登陆"><a href="#11-5-selenium的模拟登陆" class="headerlink" title="11.5.selenium的模拟登陆"></a>11.5.selenium的模拟登陆</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriverfrom time <span class="keyword">import</span> sleepbro = webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)bro.get(<span class="string">&#x27;https://qzone.qq.com/&#x27;</span>)bro.switch_to.frame(<span class="string">&#x27;login_frame&#x27;</span>)a_tag = bro.find_element_by_id(<span class="string">&#x27;switcher_plogin&#x27;</span>)a_tag.click()sleep(<span class="number">1</span>)user_name = bro.find_element_by_id(<span class="string">&#x27;u&#x27;</span>)sleep(<span class="number">1.1</span>)password = bro.find_element_by_id(<span class="string">&#x27;p&#x27;</span>)user_name.send_keys(<span class="string">&#x27;1123123&#x27;</span>)password.send_keys(<span class="string">&#x27;123123&#x27;</span>)sleep(<span class="number">1.2</span>)btn = bro.find_element_by_id(<span class="string">&#x27;login_button&#x27;</span>)btn.click()sleep(<span class="number">3</span>)bro.quit()</span><br></pre></td></tr></table></figure>

<h3 id="11-6-无头浏览器-规避检测"><a href="#11-6-无头浏览器-规避检测" class="headerlink" title="11.6.无头浏览器+规避检测"></a>11.6.无头浏览器+规避检测</h3><p>现在不少大网站对selenium采取了检测机制。比如正常情况下我们用浏览器访问淘宝等网站的<code>window.navigator.webdriver</code>的值为<code>undefined</code>。而使用selenium访问该值为true。那么如何解决这个问题呢？</p>
<p>只需要设置Chromedriver的启动参数即可。在启动Chromedriver之前，为Chrome开启实验性功能参数<code>excludeSwitches</code>，它的值为<code>[&#39;enable-automation&#39;]</code>，完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriverfrom time <span class="keyword">import</span> sleep<span class="comment"># 实现无可视化界面from selenium.webdriver.chrome.options import Options# 实现规避检测from selenium.webdriver import ChromeOptions# 实现无可视化界面的操作chrome_options = Options()chrome_options.add_argument(&#x27;--headless&#x27;)chrome_options.add_argument(&#x27;--disable-gpu&#x27;)# 实现规避检测option = ChromeOptions()option.add_experimental_option(&#x27;excludeSwitches&#x27;,[&#x27;enable-automation&#x27;])# 如何实现让selenium规避被检测到的风险# bro = webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;,chrome_options=chrome_options,options=option)bro = webdriver.Chrome(executable_path=&#x27;./chromedriver.exe&#x27;,options=option)# 五可视化界面（无头浏览器）bro.get(&#x27;https://www.baidu.com&#x27;)print(bro.page_source)sleep(2)bro.quit()</span></span><br></pre></td></tr></table></figure>



<h3 id="11-7-超级鹰的基本使用"><a href="#11-7-超级鹰的基本使用" class="headerlink" title="11.7.超级鹰的基本使用"></a>11.7.超级鹰的基本使用</h3><h2 id="12-scrapy"><a href="#12-scrapy" class="headerlink" title="12.scrapy"></a>12.scrapy</h2><h3 id="12-1-scrapy框架初识"><a href="#12-1-scrapy框架初识" class="headerlink" title="12.1.scrapy框架初识"></a>12.1.scrapy框架初识</h3><ul>
<li>什么是框架<ul>
<li>就是一个集成了很多的功能，并且 有很强通用性的一个项目模板。</li>
</ul>
</li>
<li>如何学习框架<ul>
<li>专门学习框架封装的各种功能的详细用法</li>
</ul>
</li>
<li>什么是scrapy<ul>
<li>爬虫中封装好的一个明星框架。</li>
<li>功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式。</li>
</ul>
</li>
</ul>
<h3 id="12-2-scrapy环境安装"><a href="#12-2-scrapy环境安装" class="headerlink" title="12.2.scrapy环境安装"></a>12.2.scrapy环境安装</h3><ul>
<li><p>环境安装</p>
<ul>
<li><p>linux或mac系统</p>
<ul>
<li><code>pip install scrapy</code></li>
</ul>
</li>
<li><p>windows系统</p>
<ul>
<li><p><code>pip install scrapy</code></p>
<p>测试：在终端里录入<code>scrapy</code>命令，没有报错即表示安装成功。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="12-3-scrapy基本使用"><a href="#12-3-scrapy基本使用" class="headerlink" title="12.3.scrapy基本使用"></a>12.3.scrapy基本使用</h3><ul>
<li><p>scrapy使用流程</p>
<ul>
<li><p>创建工程</p>
<ul>
<li><code>scrapy startproject ProName</code></li>
</ul>
</li>
<li><p>进入工程目录</p>
<ul>
<li><code>cd ProName</code></li>
</ul>
</li>
<li><p>创建爬虫文件</p>
<ul>
<li><p><code>scrapy genspider SpiderName www.xxx.com</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapyclass FirstSpider(scrapy.Spider):    <span class="comment"># 爬虫文件的名称，就是爬虫文件的唯一标识    name = &#x27;first&#x27;    # 允许的域名：用来限定start_urls中，哪些url可以进行请求发送    allowed_domains = [&#x27;www.baidu.com&#x27;]    # 起始的url列表：该列表中存放的url会被scrapy自动的进行请求的发送，可以有多个    start_urls = [&#x27;http://www.baidu.com/&#x27;,&#x27;http://www.sogou.com&#x27;]	# 作用于数据解析：response参数表示的就是，请求成功后对应的响应对象    # 会被调用多次，由start_urls中的元素个数决定的    def parse(self, response):        pass</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>设置<code>ROBOTSTXT=False</code></p>
</li>
<li><p>设置<code>LOG_LEVEL=&#39;ERROR&#39;</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3947.100 Safari/537.36&#x27;</span><span class="comment"># Obey robots.txt rulesROBOTSTXT_OBEY = FalseLOG_LEVEL = &#x27;ERROR&#x27;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>编写相关操作代码</p>
</li>
<li><p>执行工程</p>
<ul>
<li><code>scrapy crawl SpiderName</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="12-4-scrapy数据解析"><a href="#12-4-scrapy数据解析" class="headerlink" title="12.4.scrapy数据解析"></a>12.4.scrapy数据解析</h3><p>爬取糗事百科<a target="_blank" rel="noopener" href="https://www.qiushibaike.com/text/">https://www.qiushibaike.com/text/</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapyclass QiushiSpiderSpider(scrapy.Spider):    name = <span class="string">&#x27;qiushi_spider&#x27;</span>    <span class="comment"># allowed_domains = [&#x27;www.xxx.com&#x27;]    start_urls = [&#x27;https://www.qiushibaike.com/text/&#x27;]    def parse(self, response):        # 解析作者的名称+段子内容        div_list = response.xpath(&#x27;//div[@class=&quot;col1 old-style-col1&quot;]/div&#x27;)        # print(div_list)        for div  in div_list:            # xpath返回的是列表，但是列表元素一定是selector类型的对象            # extract可以将selector对象中，data参数的存储的字符串提取出来            # author = div.xpath(&#x27;./div[1]/a[2]/h2/text()&#x27;)[0].extract()            author = div.xpath(&#x27;./div[1]/a[2]/h2/text()&#x27;).extract_first()            # 如果列表调用了extract之后，则表示将列表中的每一个selecor对象中data对应的字符串提取了出来            content = div.xpath(&#x27;./a[1]/div/span//text()&#x27;).extract()            content = &#x27;&#x27;.join(content)            print(author,content)            break</span></span><br></pre></td></tr></table></figure>

<p>设置<code>USER_AGENT</code></p>
<p>运行<code>scrapy crawl qiushi_spider</code></p>
<h3 id="12-5-持久化存储"><a href="#12-5-持久化存储" class="headerlink" title="12.5.持久化存储"></a>12.5.持久化存储</h3><h4 id="12-5-1-基于终端指令的持久化存储"><a href="#12-5-1-基于终端指令的持久化存储" class="headerlink" title="12.5.1.基于终端指令的持久化存储"></a>12.5.1.基于终端指令的持久化存储</h4><p>只可以将parse方法的返回值存储到本地的文本文件中。</p>
<p>注意：持久化存储对应的文本文件的类型，只可以为：<code>json</code>、<code>jsonlines</code>、<code>jl</code>、<code>csv</code>、<code>xml</code>、<code>marshal</code>、<code>pickle</code></p>
<p>指令：<code>scrapy crawl qiushi_spider -o ./qiushi.csv</code></p>
<p>好处：简洁高效便捷</p>
<p>缺点：局限性比较强（数据只可以存储到指定后缀的文本文件中）</p>
<h4 id="12-5-2-基础管道持久化存储"><a href="#12-5-2-基础管道持久化存储" class="headerlink" title="12.5.2.基础管道持久化存储"></a>12.5.2.基础管道持久化存储</h4><p>编码流程：</p>
<ul>
<li><p>数据解析</p>
</li>
<li><p>在iem类中定义相关的属性</p>
</li>
<li><p>将解析的数据，封装存储到item类型的对象中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># items.pyimport scrapyclass QiushiItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    author = scrapy.Field()    content = scrapy.Field()    # pass</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将item类型的数据，提交给管道进行持久化存储的操作</p>
</li>
<li><p>在管道类的process_item中，要将其接收到的item对象中存储的数据，进行持久化存储操作</p>
<ul>
<li><p>process_item</p>
<ul>
<li>专门用来处理item类型的对象</li>
<li>该方法可以接收爬虫文件提交过来的item对象</li>
<li>该方法没接收一个item，就会被调用一次</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pipelines.pyfrom itemadapter import ItemAdapterclass QiushiPipeline:    fp = None    # 重写父类的方法，该方法只会在开始爬虫的时候，被调用一次    def open_spider(self, spider):        print(&#x27;爬虫开始...&#x27;)        self.fp = open(&#x27;./qiushi.txt&#x27;,&#x27;w&#x27;,encoding=&#x27;utf-8&#x27;)    # 专门用来处理item类型对象    def process_item(self, item, spider):        author = item[&#x27;author&#x27;]        content = item[&#x27;content&#x27;]        self.fp.write(author + &#x27;:&#x27; + content + &#x27;\n&#x27;)        return item # 这里如果写了return，则item会传递给下一个即将执行的管道类，默认都是加上    # 结束爬虫时，会被调用一次    def close_spider(self,spider):        print(&#x27;爬虫结束...&#x27;)</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>在配置文件中开启管道</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;    <span class="comment"># 数值表示优先级，数值越小，优先级越高   &#x27;qiushi.pipelines.QiushiPipeline&#x27;: 300,&#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>备注：如果有匿名用户，则会报错</p>
<ul>
<li><p>完善author的xpath：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autho = div.xpath(<span class="string">&#x27;./div[1]/a[2]/h2/text() | ./div[1]/span/h2/text()&#x27;</span>).extarct_first()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>好处：通用性强</p>
</li>
<li><p>面试题：将爬取到的 数据一份存储到本地，一本存储到数据库，如何实现？</p>
<ul>
<li><p>在管道文件中定义多个管道类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 管道文件中，一个管道类对应将一组数据存储到一个平台或一个载体中class mysqlPipeline(object):    conn = None    pool = None    value = &#x27;&#x27;    def open_spider(self, spider):        self.pool = ConnectionPool(host=&#x27;127.0.0.1&#x27;,port=6379,password=&#x27;foobared&#x27;, decode_responses=True)    def process_item(self, item, spider):        self.conn = Redis(connection_pool=self.pool)        self.conn.set(&#x27;k1&#x27;,&#x27;v1&#x27;)        value = self.conn.get(&#x27;k1&#x27;)        def close_spider(self, spider):        print(self.value)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在<code>ITEM_PIPELINES</code>中配置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;   <span class="string">&#x27;qiushi.pipelines.QiushiPipeline&#x27;</span>: <span class="number">300</span>,   <span class="string">&#x27;qiushi.pipelines.redisPipeline&#x27;</span>: <span class="number">301</span>,&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>爬虫文件提交的item，只会给管道文件中第一个被执行的管道类接受</p>
</li>
<li><p>process_item中的return item表示将item传递给下一个即将执行的管道类</p>
</li>
</ul>
</li>
</ul>
<h3 id="12-6-全站数据爬取"><a href="#12-6-全站数据爬取" class="headerlink" title="12.6.全站数据爬取"></a>12.6.全站数据爬取</h3><ul>
<li><p>基于Spider的全站数据爬取</p>
<ul>
<li><p>就是将网站中某板块下的全部页码，对应的页面数据进行爬取</p>
</li>
<li><p>需求：爬取校花网中的照片的名称</p>
</li>
<li><p>实现方式：</p>
<ul>
<li><p>将所有页面的url添加到start_urls列表（不推荐）</p>
</li>
<li><p>自行手动进行请求发送</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapyclass XiaohuaSpider(scrapy.Spider):    name = <span class="string">&#x27;xiaohua&#x27;</span>    <span class="comment"># allowed_domains = [&#x27;www.xx.com&#x27;]    start_urls = [&#x27;http://www.521609.com/tuku/index.html&#x27;]    # 生成一个通用的url模板（不可变的)    url = &#x27;http://www.521609.com/tuku/index_%d.html&#x27;    page_num = 2    def parse(self, response):        li_list = response.xpath(&#x27;/html/body/div[4]/div[3]/ul/li&#x27;)        for li in li_list:            img_name = li.xpath(&#x27;./a/p/text()&#x27;).extract_first()            print(img_name)                if self.page_num &lt;= 3:            new_url = format(self.url % self.page_num)            self.page_num += 1            # 手动发送请求，callback回调函数专门用作数据解析            yield scrapy.Request(url = new_url, callback = self.parse)</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="12-7-五大核心组件"><a href="#12-7-五大核心组件" class="headerlink" title="12.7.五大核心组件"></a>12.7.五大核心组件</h3><ul>
<li>引擎（scrapy）<ul>
<li>用来处理整个系统的数据流处理，触发事务（核心）</li>
</ul>
</li>
<li>调度器（Scheduler）<ul>
<li>用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。可以想象成要给URL（抓取网页的网址或者是链接）的优先队列，由它来决定下一个要抓取的网址是什么，同时去除重复的网址。</li>
</ul>
</li>
<li>下载器（Downloader）<ul>
<li>用于下载网页内容，并将网页内容返回给引擎，下载器是建立在<strong>twisted</strong>这个高效的异步模型上的。</li>
</ul>
</li>
<li>爬虫（Spiders）<ul>
<li>爬虫是用来干活的，用于从特定网页中提取自己需要的信息，即所谓的实体（item）。用户也可以从中提取出链接，让Scrapy继续抓取下一个页面。</li>
</ul>
</li>
<li>项目管理（Pipeline）<ul>
<li>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体，验证实体信息有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管理，并经过几个特定的次序处理数据。</li>
</ul>
</li>
</ul>
<h4 id="12-7-1-请求传参"><a href="#12-7-1-请求传参" class="headerlink" title="12.7.1.请求传参"></a>12.7.1.请求传参</h4><ul>
<li>使用场景：如果爬取解析的数据不在同一张页面中。我们就需要用到请求传参（深度爬取）</li>
<li></li>
<li>需求：爬取boss的岗位名称，岗位描述</li>
</ul>
<p>如果要使用管道进行持久化存储，需要先在item.py中定义item：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BossproItem</span>(scrapy.Item):    <span class="comment"># define the fields for your item here like:    # name = scrapy.Field()    # pass    title = scrapy.Field()    describe = scrapy.Field()</span></span><br></pre></td></tr></table></figure>

<p>然后导入item中的类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bosspro.items <span class="keyword">import</span> BossproItem</span><br></pre></td></tr></table></figure>

<p>然后在for循环中实例化item对象，把需要的字段，存到item类型的字段中：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">item = BossproItem()item[<span class="string">&#x27;title&#x27;</span>] = titleyield scrapy.Request(title_href,callback=self.parse_detail,meta=&#123;<span class="string">&#x27;item&#x27;</span>:item&#125;)</span><br></pre></td></tr></table></figure>

<p>在回调的解析方法中，接受item对象，并传入该解析方法特有的值，最后返回item给管道：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">item = response.meta[<span class="string">&#x27;item&#x27;</span>]item[<span class="string">&#x27;describe&#x27;</span>] = describeyield item</span><br></pre></td></tr></table></figure>

<ul>
<li><p>分页爬取</p>
<p>定义url模板</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">template_url = <span class="string">&#x27;http://news.longhoo.net/njxw/%d.html&#x27;</span>page_num = <span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>在首个url的parse方法中，进行分页操作</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.page_num &lt;= <span class="number">3</span>:    next_url = <span class="built_in">format</span>(self.template_url % self.page_num)    self.page_num += <span class="number">1</span>    <span class="keyword">yield</span> scrapy.Request(next_url, callback=self.parse)</span><br></pre></td></tr></table></figure>



<p>练习：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://news.longhoo.net/njxw/">http://news.longhoo.net/njxw/</a></li>
</ul>
<h4 id="12-7-2-scrapy图片爬取"><a href="#12-7-2-scrapy图片爬取" class="headerlink" title="12.7.2.scrapy图片爬取"></a>12.7.2.scrapy图片爬取</h4><ul>
<li><p>基于scrapy爬取字符串类型的数据，和爬取图片类型的数据的区别？</p>
<ul>
<li>字符串：只需要进行xpath进行解析，且提交到管道进行持久化存储。</li>
<li>图片：xpath解析出图片的src的属性值。单独的对图片地址发起请求获取图片二进制类型的数据。</li>
</ul>
</li>
<li><p>ImagePipeline：</p>
<ul>
<li>只需要将Img的src的属性值进行解析，提交到管道，管道就会对图片的src进行请求发送获取图片的二进制类型数据，且还会帮我们进行持久化存储。</li>
<li>需求：爬取图片网的图片<ul>
<li>网址：<a target="_blank" rel="noopener" href="http://www.88meitu.com/qingchun/">http://www.88meitu.com/qingchun/</a></li>
</ul>
</li>
</ul>
</li>
<li><p>使用流程：</p>
<ul>
<li>数据解析（获取图片地址）</li>
<li>在管道文件中，自定义一个基于ImagesPipeline的一个管道类<ul>
<li>get _media_request</li>
<li>file_path</li>
<li>item_completed</li>
</ul>
</li>
<li>在配置文件中：<ul>
<li>指定图片存储路径：<code>IMAGES_STORE=&#39;./imgs&#39;</code></li>
<li>指定开启的管道：自定义管道类</li>
</ul>
</li>
</ul>
</li>
<li><p>注意点：</p>
<ul>
<li><p>默认的管道类，是不处理图片格式的数据的。</p>
<p>ImagesPipeline专门用于文件下的管道类，下载过程支持异步和多线程</p>
<p>重写父类的三个方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define your item pipelines here## Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html# useful for handling different item types with a single interfacefrom itemadapter import ItemAdapterfrom scrapy.pipelines.images import ImagesPipelineimport scrapy# class GirlpicPipeline:#     def process_item(self, item, spider):#         return itemfrom girlpic.items import GirlpicItemclass girlPipeline(ImagesPipeline):    # 对item中的图片进行请求操作    def get_media_requests(self, item, info):        # return super().get_media_requests(item, info)        yield scrapy.Request(item[&#x27;src&#x27;])    # 定制图片的名称    # def file_path(self, request, response, info, *, item):        # return super().file_path(request, response=response, info=info, item=item)    def file_path(self, request, response=None, info=None):        item = GirlpicItem()        image_name = item[&#x27;image_name&#x27;]        return image_name    #     def item_completed(self, results, item, info):        # return super().item_completed(results, item, info)        # 返回给下一个即将被执行的管道类中        return item</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>在settings.py中，定义图片的存储目录</p>
<p><code>IMAGES_STORE = &#39;./imgs&#39;</code></p>
<p>如果路径不存在，则会自行创建</p>
</li>
<li><p>在settings.py中，开启指定的管道类</p>
</li>
</ul>
</li>
<li><p>图片懒加载</p>
<ul>
<li>有些图片的src属性，写成src2之类的，只有元素被滑动到可视窗口中是，才会切换成src属性。</li>
<li>在xpath提取的时候，得提取src2属性。</li>
<li>有的图片懒加载，变化的是是src的值，这时候得用其他属性，直接用src属性，可能会有问题。</li>
</ul>
</li>
</ul>
<h4 id="12-7-3-中间件"><a href="#12-7-3-中间件" class="headerlink" title="12.7.3.中间件"></a>12.7.3.中间件</h4><h5 id="12-7-3-1-中间件初始"><a href="#12-7-3-1-中间件初始" class="headerlink" title="12.7.3.1.中间件初始"></a>12.7.3.1.中间件初始</h5><ul>
<li>下载中间件<ul>
<li>位置：引擎和下载器之间</li>
<li>作用：批量拦截到整个工程中所有的请求和响应</li>
<li>拦截请求：<ul>
<li>UA伪装</li>
<li>代理IP</li>
</ul>
</li>
<li>拦截响应：<ul>
<li>篡改响应数据，响应对象</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="12-7-3-2-中间件-处理请求"><a href="#12-7-3-2-中间件-处理请求" class="headerlink" title="12.7.3.2.中间件-处理请求"></a>12.7.3.2.中间件-处理请求</h5><ul>
<li><p>爬虫中间件</p>
</li>
<li><p>下载中间件</p>
<ul>
<li><p>process_request</p>
<ul>
<li><p>用来拦截请求</p>
</li>
<li><p>UA伪装</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_request</span>(<span class="params">self, request, spider</span>):    request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = random.choice(self.user_agent_list)    request.meta[<span class="string">&#x27;proxy&#x27;</span>] = <span class="string">&#x27;http://183.151.202.233:9999&#x27;</span>    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>process_response</p>
<ul>
<li>用来<strong>所有的</strong>拦截响应</li>
<li>需求：爬取网易新闻中的新闻数据（标题和内容）<ul>
<li>1.通过网易新闻</li>
</ul>
</li>
</ul>
</li>
<li><p>process_exception</p>
<ul>
<li><p>用来拦截发生异常的请求对象</p>
<ul>
<li><p>代理ip：process_exception:return request</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_exception</span>(<span class="params">self, request, exception, spider</span>):    <span class="comment"># 代理    if request.url.split(&#x27;:&#x27;)[0] == &#x27;http&#x27;:        request.meta[&#x27;proxy&#x27;] = &#x27;http:&#x27; + random.choice(self.PROXY_http)    else:        request.meta[&#x27;proxy&#x27;] = &#x27;https:&#x27; + random.choice(self.PROXY_https)    # 将修正后的请求对象，重新发送    return request</span></span><br></pre></td></tr></table></figure>
</li>
<li></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>settings.py中开启中间件</p>
</li>
</ul>
<h4 id="12-7-4-中间件-处理响应"><a href="#12-7-4-中间件-处理响应" class="headerlink" title="12.7.4.中间件-处理响应"></a>12.7.4.中间件-处理响应</h4><p>案例：网易新闻</p>
<ul>
<li><p>通过网易新闻的首页解析五大板块对应的详情页的url(没有动态加载)</p>
</li>
<li><p>每一个板块对应的新闻标题都是动态加载的出来的（动态加载）</p>
<ul>
<li><p>使用下载中间件的process_response，篡改响应对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_response</span>(<span class="params">self, request, response, spider</span>):        bro = spider.bro        <span class="comment"># 挑选指定的响应对象进行篡改        # 通过传入的url和爬虫中的存的url进行判断        # 匹配上的话，就篡改对应的response对象        if request.url in spider.category_urls:            # 传入目标板块的response            # 针对定位的response进行篡改            # 实例化一个新的响应对象（符合需求，包含动态加载出来的新闻数据），替代原来旧的响应对象            # 基于selenium获取动态加载数据            bro.get(request.url)            sleep(2)            page_text = bro.page_source            new_response = HtmlResponse(url=request.url, body=page_text, encoding=&#x27;utf-8&#x27;, request=request)            return new_response        else:            return response</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>在爬虫文件的init方法中，实例化浏览器对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):    self.bro = webdriver.Chrome(executable_path=<span class="string">&#x27;./chromedriver.exe&#x27;</span>)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>通过解析出每一新闻详情页的url获取详情页的页面源码，解析出新闻内容</p>
</li>
</ul>
<h4 id="12-7-5-crawlspider的全站数据爬取"><a href="#12-7-5-crawlspider的全站数据爬取" class="headerlink" title="12.7.5.crawlspider的全站数据爬取"></a>12.7.5.crawlspider的全站数据爬取</h4><p>Crawlspider：Spider的一个子类</p>
<ul>
<li>全站数据爬取的方式<ul>
<li>基于Spider：手动请求</li>
<li>基于CrawlSpider</li>
</ul>
</li>
<li>CrawlSpider的使用：<ul>
<li>创建一个工程</li>
<li>cd xxx</li>
<li>创建爬虫文件(CrawlSpider)<ul>
<li><code>scrapy genspider -t crawl xxx www.xxx.com</code></li>
</ul>
</li>
<li>启动爬虫文件<ul>
<li><code>scrapy crawl 文件名	 </code></li>
</ul>
</li>
</ul>
</li>
<li>链接提取器<ul>
<li>根据指定规则<code>(allow=&quot;正则&quot;)</code>进行执行链接的提取</li>
<li>注意特殊字符，在正则表达式中的转义，比如<code>?</code></li>
</ul>
</li>
<li>规则解析器<ul>
<li>将链接提取器提取到的链接，进行指定规则<code>(callback)</code>的解析操作</li>
<li>follow&#x3D;True，则意味着对解析出来的url页面，再重复按照相同的规则进行提取，并去重。</li>
</ul>
</li>
</ul>
<h5 id="12-7-5-1-练习"><a href="#12-7-5-1-练习" class="headerlink" title="12.7.5.1.练习"></a>12.7.5.1.练习</h5><ul>
<li><a target="_blank" rel="noopener" href="https://wz.sun0769.com/political/index/politicsNewest?id=1&page=0">https://wz.sun0769.com/political/index/politicsNewest?id=1&amp;page=0</a></li>
<li>爬取编号，新闻标题，内容</li>
<li>二级页编号要多爬取一次</li>
<li>可以设置多个链接提取器</li>
</ul>
<h3 id="12-8-分布式"><a href="#12-8-分布式" class="headerlink" title="12.8.分布式"></a>12.8.分布式</h3><h4 id="12-8-1-分布式概述"><a href="#12-8-1-分布式概述" class="headerlink" title="12.8.1.分布式概述"></a>12.8.1.分布式概述</h4><p>分布式爬虫</p>
<ul>
<li>概念：我们需要搭建一个分布式的集群，让其对一组资源进行分布联合爬取。</li>
<li>作用：提升爬取效率</li>
</ul>
<p>如何实现分布式</p>
<ul>
<li>安装一个scrapy-redis组件</li>
<li>原生的scrapy是不可以实现分布式爬虫的，必须要让scrapy结合着scrapy-redis组件，一起实现分布式爬虫。</li>
</ul>
<h4 id="12-8-2-分布式搭建"><a href="#12-8-2-分布式搭建" class="headerlink" title="12.8.2.分布式搭建"></a>12.8.2.分布式搭建</h4><ul>
<li><p>创建一个工程</p>
</li>
<li><p>创建一个基于CrawlSpider的爬虫文件</p>
</li>
<li><p>修改当前的爬虫文件</p>
<ul>
<li>导包：<code>from scrapy_redis.spiders import RedisCrawlSpider</code></li>
<li>将<code>strat_urls</code>和<code>allowed_domains</code>进行注释</li>
<li>添加一个新属性：<code>redis_key = &#39;sun&#39;</code>，表示可以被共享的调度器队列的名称</li>
<li>编写数据解析相关的操作</li>
<li>将当前爬虫类的父类修改成RedisCrawlSpider</li>
</ul>
</li>
<li><p>修改配置文件，末尾添加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定管道ITEM_PIPELINES = &#123;    &#x27;scrapy_redis.pipelines.RedisPipeline&#x27; : 400&#125;# 指定调度器# 增加了一个去重容器类的配置，作用使用Redis集合来存储请求的指纹数据，从而实现请求去重的持久化DUPEFILTER_CLASS =&#x27;scrapy_redis.dupefilter.RFPDupeFilter&#x27;# 使用scrapy_redis组件自己的调度器SCHEDULER = &#x27;scrapy_redis.scheduler.Scheduler&#x27;# 配置调度器是否要持久化，也就是当爬虫结束了，要不要清空Redis中请求队列和去重指纹的set。如果是,TrueSCHEDULER_PERSIST = True# 指定redisREDIS_HOST = &#x27;127.0.0.1&#x27;REDIS_PORT = &#x27;6379&#x27;REDIS_PARAMS = &#123;     &#x27;password&#x27;: &#x27;123123&#x27;, &#125;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>redis相关配置操作</p>
<ul>
<li><p>若是云主机，在控制台开启对应端口</p>
</li>
<li><p>注释 bind</p>
</li>
<li><p>关闭保护模式：protected mode &#x3D; no</p>
</li>
<li><p>后台启动redis</p>
</li>
<li><p>启动客户端</p>
</li>
</ul>
</li>
<li><p>执行工程</p>
<ul>
<li>进入到爬虫文件所在的目录，执行<code>scrapy runspider xxx.py</code></li>
<li>向调度器的队列中，放入起始url<ul>
<li>调度器的队列在redis的客户端中<ul>
<li><code>lpush key url</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>爬取到的数据存储在了redis的proName:items这个数据结构中</p>
</li>
</ul>
<h4 id="12-8-3-增量式爬虫"><a href="#12-8-3-增量式爬虫" class="headerlink" title="12.8.3.增量式爬虫"></a>12.8.3.增量式爬虫</h4><ul>
<li>概念：监测网站数据实时更新的情况，只会爬取网站最新更新出来的数据</li>
<li>分析：<ul>
<li>指定一个起始url</li>
<li>基于CrawlSpider获取其他页码链接</li>
<li>基于Rule将其他页码链接进行请求</li>
<li>从每一个页码对应的页面源码中，解析出每一个电影详情页的url</li>
</ul>
</li>
<li>核心：<ul>
<li>检测电影详情页的url之前有没有被请求过</li>
<li>将爬取过的电影详情页的url存储</li>
</ul>
</li>
<li>对详情页的url发起请求，然后解析出电影名称和简介</li>
<li>进行持久化存储</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/Saiable">Sai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://mindcons.cn/2023/11/13/md/python/spider/%E7%88%AC%E8%99%AB%E5%9F%BA%E7%A1%80/">https://mindcons.cn/2023/11/13/md/python/spider/爬虫基础/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://mindcons.cn" target="_blank">吕小布の博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://bu.dusays.com/2023/02/10/63e580229ee01.webp" target="_blank"><img class="post-qr-code-img" src="https://bu.dusays.com/2023/02/10/63e580229ee01.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://bu.dusays.com/2023/02/09/63e48775d4f42.webp" target="_blank"><img class="post-qr-code-img" src="https://bu.dusays.com/2023/02/09/63e48775d4f42.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/11/13/md/python/spider/%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/"><img class="prev-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2023/11/13/md/python/spider/%E6%96%B0%E9%97%BB%E8%A1%8C%E4%B8%9A%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info"></div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://bu.dusays.com/2023/02/10/63e583843360e.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Sai</div><div class="author-info__description">天可补,海可填,南山可移。日月既往,不可复追。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">178</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">52</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Saiable"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://bu.dusays.com/2023/02/10/63e57e822f198.webp" target="_blank" title="QQ"><i class="fa-brands fa-qq"></i></a><a class="social-icon" href="https://bu.dusays.com/2023/02/10/63e57f8ac9fae.webp" target="_blank" title="WeChat"><i class="fa-brands fa-weixin"></i></a><a class="social-icon" href="https://github.com/Saiable" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mindcons@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">日月既往,不可复追。</div></div><div class="xpand" style="height:200px;"><canvas class="illo" width="800" height="800" style="max-width: 200px; max-height: 200px; touch-action: none; width: 640px; height: 640px;"></canvas></div><script src="https://npm.elemecdn.com/ethan4116-blog/lib/js/other/two-people/twopeople1.js"></script><script src="https://npm.elemecdn.com/ethan4116-blog/lib/js/other/two-people/zdog.dist.js"></script><script id="rendered-js" src="https://npm.elemecdn.com/ethan4116-blog/lib/js/other/two-people/twopeople.js"></script><style>.card-widget.card-announcement {
margin: 0;
align-items: center;
justify-content: center;
text-align: center;
}
canvas {
display: block;
margin: 0 auto;
cursor: move;
}</style><div class="card-widget tzy-right-widget" id="card-wechat"><div id="flip-wrapper"><div id="flip-content"><div class="face"></div><div class="back face"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%88%AC%E8%99%AB%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">1.爬虫简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%88%AC%E8%99%AB%E5%90%88%E6%B3%95%E6%80%A7%E6%8E%A2%E7%A9%B6"><span class="toc-number">1.1.</span> <span class="toc-text">1.1.爬虫合法性探究</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E7%88%AC%E8%99%AB%E7%A9%B6%E7%AB%9F%E6%98%AF%E8%BF%9D%E6%B3%95%E8%BF%98%E6%98%AF%E5%90%88%E6%B3%95%E7%9A%84%EF%BC%9F"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1.1.爬虫究竟是违法还是合法的？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E7%88%AC%E8%99%AB%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%A3%8E%E9%99%A9"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.1.2.爬虫带来的风险</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-3-%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E8%BF%9B%E5%B1%80%E5%AD%90%E5%96%9D%E8%8C%B6"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.1.3.如何避免进局子喝茶</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%88%AC%E8%99%AB%E5%88%9D%E5%A7%8B%E6%B7%B1%E5%85%A5"><span class="toc-number">1.2.</span> <span class="toc-text">1.2.爬虫初始深入</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-1-%E7%88%AC%E8%99%AB%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%AD%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.2.1.爬虫在使用场景中的分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E7%88%AC%E8%99%AB%E7%9A%84%E7%9F%9B%E4%B8%8E%E7%9B%BE"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2.2.爬虫的矛与盾</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-robots-txt%E5%8D%8F%E8%AE%AE"><span class="toc-number">1.2.3.</span> <span class="toc-text">1.2.3.robots.txt协议</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-http-amp-https%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.</span> <span class="toc-text">2.http&amp;https协议</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-http%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.1.</span> <span class="toc-text">2.1.http协议</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-https%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.2.</span> <span class="toc-text">2.2.https协议</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Requests%E6%A8%A1%E5%9D%97"><span class="toc-number">3.</span> <span class="toc-text">3.Requests模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Requests%E5%B7%A9%E5%9B%BA"><span class="toc-number">3.1.</span> <span class="toc-text">3.1.Requests巩固</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-%E6%B7%B1%E5%85%A5%E6%A1%88%E4%BE%8B%E4%BB%8B%E7%BB%8D"><span class="toc-number">3.1.1.</span> <span class="toc-text">3.1.1.深入案例介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-2-%E7%AE%80%E6%98%93%E7%BD%91%E9%A1%B5%E9%87%87%E9%9B%86%E5%99%A8"><span class="toc-number">3.1.2.</span> <span class="toc-text">3.1.2.简易网页采集器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-3-%E7%99%BE%E5%BA%A6%E7%BF%BB%E8%AF%91"><span class="toc-number">3.1.3.</span> <span class="toc-text">3.1.3.百度翻译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-4-%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1"><span class="toc-number">3.1.4.</span> <span class="toc-text">3.1.4.豆瓣电影</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-5-%E4%BD%9C%E4%B8%9A"><span class="toc-number">3.1.5.</span> <span class="toc-text">3.1.5.作业</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%BB%BC%E5%90%88%E7%BB%83%E4%B9%A0"><span class="toc-number">3.2.</span> <span class="toc-text">3.2.综合练习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-%E8%8D%AF%E7%9B%91%E6%80%BB%E5%B1%8001"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1.药监总局01</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90%E6%A6%82%E8%BF%B0"><span class="toc-number">4.</span> <span class="toc-text">4.数据解析概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%9B%BE%E7%89%87%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">4.1.</span> <span class="toc-text">4.1.图片数据爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%AD%A3%E5%88%99%E8%A7%A3%E6%9E%90"><span class="toc-number">4.2.</span> <span class="toc-text">4.2.正则解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-bs4%E8%A7%A3%E6%9E%90"><span class="toc-number">4.3.</span> <span class="toc-text">4.3.bs4解析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-1-bs4%E8%A7%A3%E6%9E%90%E6%A6%82%E8%BF%B0"><span class="toc-number">4.3.1.</span> <span class="toc-text">4.3.1.bs4解析概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-2-bs4%E8%A7%A3%E6%9E%90%E5%85%B7%E4%BD%93%E4%BD%BF%E7%94%A8%E8%AE%B2%E8%A7%A3"><span class="toc-number">4.3.2.</span> <span class="toc-text">4.3.2.bs4解析具体使用讲解</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-3-3-bs4%E8%A7%A3%E6%9E%90%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%88%98"><span class="toc-number">4.3.3.</span> <span class="toc-text">4.3.3.bs4解析案例实战</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-xpath%E8%A7%A3%E6%9E%90"><span class="toc-number">4.4.</span> <span class="toc-text">4.4.xpath解析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-xpath%E8%A7%A3%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-number">4.4.1.</span> <span class="toc-text">4.4.1.xpath解析基础</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-2-xpath%E5%AE%9E%E6%88%98"><span class="toc-number">4.4.2.</span> <span class="toc-text">4.4.2.xpath实战</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-2-1-xpath-58%E4%BA%8C%E6%89%8B%E6%88%BF"><span class="toc-number">4.4.2.1.</span> <span class="toc-text">4.4.2.1.xpath-58二手房</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-2-2-xpath-4k%E5%9B%BE%E7%89%87%E8%A7%A3%E6%9E%90%E4%B8%8B%E8%BD%BD"><span class="toc-number">4.4.2.2.</span> <span class="toc-text">4.4.2.2.xpath-4k图片解析下载</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-2-3-xpath-%E5%85%A8%E5%9B%BD%E5%9F%8E%E5%B8%82%E5%90%8D%E7%A7%B0%E7%88%AC%E5%8F%96"><span class="toc-number">4.4.2.3.</span> <span class="toc-text">4.4.2.3.xpath-全国城市名称爬取</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-4-2-4-%E4%BD%9C%E4%B8%9A"><span class="toc-number">4.4.2.4.</span> <span class="toc-text">4.4.2.4.作业</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB"><span class="toc-number">5.</span> <span class="toc-text">5.验证码识别</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%E7%AE%80%E4%BB%8B"><span class="toc-number">5.1.</span> <span class="toc-text">5.1.验证码识别简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BA%91%E6%89%93%E7%A0%81%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">5.2.云打码使用流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-%E4%BA%91%E6%89%93%E7%A0%81%E5%B9%B3%E5%8F%B0"><span class="toc-number">5.2.1.</span> <span class="toc-text">5.2.1.云打码平台</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-Tesseract-OCR%E5%AE%8C%E6%88%90%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AE%AD%E7%BB%83"><span class="toc-number">5.2.2.</span> <span class="toc-text">5.2.2.Tesseract-OCR完成验证码训练</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%8F%A4%E8%AF%97%E6%96%87%E7%BD%91%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB"><span class="toc-number">5.3.</span> <span class="toc-text">5.3.古诗文网验证码识别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86"><span class="toc-number">6.</span> <span class="toc-text">6.模拟登陆</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86%E5%AE%9E%E7%8E%B0%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">6.1.模拟登陆实现流程梳理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86"><span class="toc-number">6.2.</span> <span class="toc-text">6.2模拟登陆</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86cookie%E6%93%8D%E4%BD%9C"><span class="toc-number">6.3.</span> <span class="toc-text">6.3.模拟登陆cookie操作</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E4%BB%A3%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">7.代理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E4%BB%A3%E7%90%86%E7%90%86%E8%AE%BA%E8%AE%B2%E8%A7%A3"><span class="toc-number">7.1.</span> <span class="toc-text">7.1.代理理论讲解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E4%BB%A3%E7%90%86%E5%9C%A8%E7%88%AC%E8%99%AB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">7.2.</span> <span class="toc-text">7.2.代理在爬虫中的应用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-number">8.</span> <span class="toc-text">8.异步爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-1-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB%E6%A6%82%E8%BF%B0"><span class="toc-number">8.1.</span> <span class="toc-text">8.1.异步爬虫概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB-%E5%A4%9A%E8%BF%9B%E7%A8%8B-amp-%E5%A4%9A%E7%BA%BF%E7%A8%8B"><span class="toc-number">8.2.</span> <span class="toc-text">8.2.异步爬虫-多进程&amp;多线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-3-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB-%E8%BF%9B%E7%A8%8B%E6%B1%A0-amp-%E7%BA%BF%E7%A8%8B%E6%B1%A0"><span class="toc-number">8.3.</span> <span class="toc-text">8.3.异步爬虫-进程池&amp;线程池</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-4-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB-%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%A1%88%E4%BE%8B%E5%BA%94%E7%94%A8"><span class="toc-number">8.4.</span> <span class="toc-text">8.4.异步爬虫-线程池案例应用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-%E5%8D%8F%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-number">8.5.</span> <span class="toc-text">9.1.协程相关概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-1-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B"><span class="toc-number">8.5.1.</span> <span class="toc-text">9.1.1.异步编程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-2-%E5%8D%8F%E7%A8%8B"><span class="toc-number">8.5.2.</span> <span class="toc-text">9.1.2.协程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-2-1-greenlet%E5%AE%9E%E7%8E%B0%E5%8D%8F%E7%A8%8B"><span class="toc-number">8.5.2.1.</span> <span class="toc-text">9.1.2.1.greenlet实现协程</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-2-2-yield%E5%85%B3%E9%94%AE%E5%AD%97"><span class="toc-number">8.5.2.2.</span> <span class="toc-text">9.1.2.2.yield关键字</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-2-3-asyncio"><span class="toc-number">8.5.2.3.</span> <span class="toc-text">9.1.2.3.asyncio</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-2-3-async-amp-await%E5%85%B3%E9%94%AE%E5%AD%97"><span class="toc-number">8.5.2.4.</span> <span class="toc-text">9.1.2.3.async &amp; await关键字</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-3-%E5%8D%8F%E7%A8%8B%E6%84%8F%E4%B9%89"><span class="toc-number">8.5.3.</span> <span class="toc-text">9.1.3.协程意义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9-1-4-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B"><span class="toc-number">8.5.4.</span> <span class="toc-text">9.1.4.异步编程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-1asyncio%E4%BA%8B%E4%BB%B6%E5%BE%AA%E7%8E%AF"><span class="toc-number">8.5.4.1.</span> <span class="toc-text">9.1.4.1asyncio事件循环</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-2-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B"><span class="toc-number">8.5.4.2.</span> <span class="toc-text">9.1.4.2.快速上手</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-3-await%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">8.5.4.3.</span> <span class="toc-text">9.1.4.3.await关键词</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-4-task%E5%AF%B9%E8%B1%A1"><span class="toc-number">8.5.4.4.</span> <span class="toc-text">9.1.4.4.task对象</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-5-async%E7%9A%84future%E5%AF%B9%E8%B1%A1"><span class="toc-number">8.5.4.5.</span> <span class="toc-text">9.1.4.5.async的future对象</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-6-concurrent%E7%9A%84future%E5%AF%B9%E8%B1%A1"><span class="toc-number">8.5.4.6.</span> <span class="toc-text">9.1.4.6.concurrent的future对象</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-7-%E5%BC%82%E6%AD%A5%E5%92%8C%E9%9D%9E%E5%BC%82%E6%AD%A5%E6%B7%B7%E5%90%88%E6%A1%88%E4%BE%8B"><span class="toc-number">8.5.4.7.</span> <span class="toc-text">9.1.4.7.异步和非异步混合案例</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-8-%E5%BC%82%E6%AD%A5%E8%BF%AD%E4%BB%A3%E5%99%A8"><span class="toc-number">8.5.4.8.</span> <span class="toc-text">9.1.4.8.异步迭代器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-9-%E5%BC%82%E6%AD%A5%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AE%A1%E7%90%86%E5%99%A8"><span class="toc-number">8.5.4.9.</span> <span class="toc-text">9.1.4.9.异步上下文管理器</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-10-uvloop"><span class="toc-number">8.5.4.10.</span> <span class="toc-text">9.1.4.10.uvloop</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-11-%E6%A1%88%E4%BE%8B-%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9Credis"><span class="toc-number">8.5.4.11.</span> <span class="toc-text">9.1.4.11.案例-异步操作redis</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-12-%E6%A1%88%E4%BE%8B-%E5%BC%82%E6%AD%A5%E6%93%8D%E4%BD%9Cmysql"><span class="toc-number">8.5.4.12.</span> <span class="toc-text">9.1.4.12.案例-异步操作mysql</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-13-FastApi%E6%A1%86%E6%9E%B6%E5%BC%82%E6%AD%A5"><span class="toc-number">8.5.4.13.</span> <span class="toc-text">9.1.4.13.FastApi框架异步</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-14-%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-number">8.5.4.14.</span> <span class="toc-text">9.1.4.14.异步爬虫</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#9-1-4-15-%E6%80%BB%E7%BB%93"><span class="toc-number">8.5.4.15.</span> <span class="toc-text">9.1.4.15.总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-%E5%8D%8F%E7%A8%8B%E7%9B%B8%E5%85%B3%E6%93%8D%E4%BD%9C"><span class="toc-number">8.6.</span> <span class="toc-text">9.2.协程相关操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-number">8.7.</span> <span class="toc-text">9.3.多任务异步协程，实现异步爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-aiohttp%E6%A8%A1%E5%9D%97"><span class="toc-number">9.</span> <span class="toc-text">10.aiohttp模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-1-aiohttp%E6%A8%A1%E5%9D%97%E5%BC%95%E5%87%BA"><span class="toc-number">9.1.</span> <span class="toc-text">10.1.aiohttp模块引出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-2-aiohttp-%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%BC%82%E6%AD%A5%E5%8D%8F%E7%A8%8B%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E7%88%AC%E8%99%AB"><span class="toc-number">9.2.</span> <span class="toc-text">10.2.aiohttp+多任务异步协程，实现异步爬虫</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#11-selenium"><span class="toc-number">10.</span> <span class="toc-text">11.selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-1-selenium%E7%AE%80%E4%BB%8B"><span class="toc-number">10.1.</span> <span class="toc-text">11.1.selenium简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-2-selenium%E5%88%9D%E8%AF%95"><span class="toc-number">10.2.</span> <span class="toc-text">11.2.selenium初试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-3-selenium%E5%85%B6%E4%BB%96%E8%87%AA%E5%8A%A8%E5%8C%96%E6%93%8D%E4%BD%9C"><span class="toc-number">10.3.</span> <span class="toc-text">11.3.selenium其他自动化操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-4-iframe%E5%A4%84%E7%90%86-%E5%8A%A8%E4%BD%9C%E9%93%BE"><span class="toc-number">10.4.</span> <span class="toc-text">11.4.iframe处理+动作链</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-5-selenium%E7%9A%84%E6%A8%A1%E6%8B%9F%E7%99%BB%E9%99%86"><span class="toc-number">10.5.</span> <span class="toc-text">11.5.selenium的模拟登陆</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-6-%E6%97%A0%E5%A4%B4%E6%B5%8F%E8%A7%88%E5%99%A8-%E8%A7%84%E9%81%BF%E6%A3%80%E6%B5%8B"><span class="toc-number">10.6.</span> <span class="toc-text">11.6.无头浏览器+规避检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-7-%E8%B6%85%E7%BA%A7%E9%B9%B0%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">10.7.</span> <span class="toc-text">11.7.超级鹰的基本使用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#12-scrapy"><span class="toc-number">11.</span> <span class="toc-text">12.scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#12-1-scrapy%E6%A1%86%E6%9E%B6%E5%88%9D%E8%AF%86"><span class="toc-number">11.1.</span> <span class="toc-text">12.1.scrapy框架初识</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-2-scrapy%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="toc-number">11.2.</span> <span class="toc-text">12.2.scrapy环境安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-3-scrapy%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">11.3.</span> <span class="toc-text">12.3.scrapy基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-4-scrapy%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-number">11.4.</span> <span class="toc-text">12.4.scrapy数据解析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-5-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-number">11.5.</span> <span class="toc-text">12.5.持久化存储</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-5-1-%E5%9F%BA%E4%BA%8E%E7%BB%88%E7%AB%AF%E6%8C%87%E4%BB%A4%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-number">11.5.1.</span> <span class="toc-text">12.5.1.基于终端指令的持久化存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-5-2-%E5%9F%BA%E7%A1%80%E7%AE%A1%E9%81%93%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-number">11.5.2.</span> <span class="toc-text">12.5.2.基础管道持久化存储</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-6-%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">11.6.</span> <span class="toc-text">12.6.全站数据爬取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-7-%E4%BA%94%E5%A4%A7%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">11.7.</span> <span class="toc-text">12.7.五大核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-1-%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82"><span class="toc-number">11.7.1.</span> <span class="toc-text">12.7.1.请求传参</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-2-scrapy%E5%9B%BE%E7%89%87%E7%88%AC%E5%8F%96"><span class="toc-number">11.7.2.</span> <span class="toc-text">12.7.2.scrapy图片爬取</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-3-%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">11.7.3.</span> <span class="toc-text">12.7.3.中间件</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#12-7-3-1-%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%88%9D%E5%A7%8B"><span class="toc-number">11.7.3.1.</span> <span class="toc-text">12.7.3.1.中间件初始</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#12-7-3-2-%E4%B8%AD%E9%97%B4%E4%BB%B6-%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82"><span class="toc-number">11.7.3.2.</span> <span class="toc-text">12.7.3.2.中间件-处理请求</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-4-%E4%B8%AD%E9%97%B4%E4%BB%B6-%E5%A4%84%E7%90%86%E5%93%8D%E5%BA%94"><span class="toc-number">11.7.4.</span> <span class="toc-text">12.7.4.中间件-处理响应</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-7-5-crawlspider%E7%9A%84%E5%85%A8%E7%AB%99%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96"><span class="toc-number">11.7.5.</span> <span class="toc-text">12.7.5.crawlspider的全站数据爬取</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#12-7-5-1-%E7%BB%83%E4%B9%A0"><span class="toc-number">11.7.5.1.</span> <span class="toc-text">12.7.5.1.练习</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-8-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-number">11.8.</span> <span class="toc-text">12.8.分布式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#12-8-1-%E5%88%86%E5%B8%83%E5%BC%8F%E6%A6%82%E8%BF%B0"><span class="toc-number">11.8.1.</span> <span class="toc-text">12.8.1.分布式概述</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-8-2-%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA"><span class="toc-number">11.8.2.</span> <span class="toc-text">12.8.2.分布式搭建</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#12-8-3-%E5%A2%9E%E9%87%8F%E5%BC%8F%E7%88%AC%E8%99%AB"><span class="toc-number">11.8.3.</span> <span class="toc-text">12.8.3.增量式爬虫</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/06/md/data/%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%E4%B8%8E%E8%BF%9B%E9%98%B6/" title="无题"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2023/12/06/md/data/%E6%95%B0%E6%8D%AE%E7%BB%9F%E8%AE%A1%E4%B8%8E%E5%88%86%E6%9E%90/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80%E4%B8%8E%E8%BF%9B%E9%98%B6/" title="无题">无题</a><time datetime="2023-12-05T23:08:14.736Z" title="发表于 2023-12-06 07:08:14">2023-12-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/01/md/data/%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8/ppocrlabel/" title="ppocrlabel安装及使用">ppocrlabel安装及使用</a><time datetime="2023-11-30T22:12:44.754Z" title="发表于 2023-12-01 06:12:44">2023-12-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/01/md/data/%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8/labelStudio%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/" title="labelStudio安装及使用">labelStudio安装及使用</a><time datetime="2023-11-30T22:12:44.739Z" title="发表于 2023-12-01 06:12:44">2023-12-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/01/md/data/%E6%95%B0%E6%8D%AE%E6%A0%87%E6%B3%A8/brat%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4/" title="brat安装步骤及注意事项">brat安装步骤及注意事项</a><time datetime="2023-11-30T22:12:44.726Z" title="发表于 2023-12-01 06:12:44">2023-12-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/13/md/linux/%E8%BF%90%E7%BB%B4/Neovim/neovim%E5%AE%9E%E6%93%8D/" title="Neovim 实操">Neovim 实操</a><time datetime="2023-11-12T23:58:07.758Z" title="发表于 2023-11-13 07:58:07">2023-11-13</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://bu.dusays.com/2023/02/10/63e57cef87293.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 <i id="heartbeat" class="fa fas fa-heartbeat"></i> Sai</div><a href="https://icp.gov.moe/?keyword=20230203" target="_blank">萌ICP备20230203号</a><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/"><img src="https://bu.dusays.com/2023/02/09/63e488a78b827.webp" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://bu.dusays.com/2023/02/10/63e57f321880a.webp" title="主题采用butterfly"></a></p></div></div><link rel="stylesheet" href="/custom/css/heartbeat.min.css" ></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="聊天"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div><div class="rightMenu-item" id="menu-print"><i class="fa-solid fa-print fa-fw"></i><span>打印页面</span></div></div></div><div id="rightmenu-mask"></div><div id="myscoll"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo.mindcons.cn/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo.mindcons.cn/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><canvas id="universe"></canvas><script defer src="/custom/js/universe.js"></script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/custom/js/rightMenu.js"></script><script defer data-pjax src="/custom/js/cat.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "717070d8-e449-4d60-b9b6-c1421478f42c";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (true) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (true) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>